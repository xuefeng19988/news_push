#!/usr/bin/env python3
"""
æµ‹è¯•æ–°é—»æºæœ‰æ•ˆæ€§
æ£€æŸ¥æ‰€æœ‰RSS/APIæºæ˜¯å¦å¯ç”¨
"""

import feedparser
import requests
import time
from datetime import datetime
from typing import Dict, List, Tuple
import json

def test_rss_source(url: str, name: str) -> Tuple[bool, str, int]:
    """
    æµ‹è¯•RSSæº
    
    Args:
        url: RSS URL
        name: æ–°é—»æºåç§°
        
    Returns:
        (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯/æˆåŠŸä¿¡æ¯, æ–‡ç« æ•°é‡)
    """
    try:
        start_time = time.time()
        
        # è®¾ç½®è¶…æ—¶å’ŒUser-Agent
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ä½¿ç”¨feedparserè§£æ
        feed = feedparser.parse(url)
        
        elapsed = time.time() - start_time
        
        if feed.bozo:  # è§£æé”™è¯¯
            error_msg = str(feed.bozo_exception)
            return False, f"è§£æé”™è¯¯: {error_msg}", 0
        
        if not feed.entries:
            return False, "æ— æ–‡ç« å†…å®¹", 0
        
        article_count = len(feed.entries)
        
        # æ£€æŸ¥æ–‡ç« è´¨é‡
        valid_articles = 0
        for entry in feed.entries[:5]:  # æ£€æŸ¥å‰5ç¯‡æ–‡ç« 
            if hasattr(entry, 'title') and entry.title and hasattr(entry, 'link') and entry.link:
                valid_articles += 1
        
        if valid_articles == 0:
            return False, "æ–‡ç« æ ¼å¼æ— æ•ˆ", 0
        
        return True, f"æˆåŠŸè·å– {article_count} ç¯‡æ–‡ç«  ({elapsed:.1f}ç§’)", article_count
        
    except Exception as e:
        return False, f"å¼‚å¸¸: {str(e)}", 0

def test_api_source(url: str, name: str) -> Tuple[bool, str, int]:
    """
    æµ‹è¯•APIæº
    
    Args:
        url: API URL
        name: æ–°é—»æºåç§°
        
    Returns:
        (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯/æˆåŠŸä¿¡æ¯, æ•°æ®æ•°é‡)
    """
    try:
        start_time = time.time()
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        elapsed = time.time() - start_time
        
        if response.status_code != 200:
            return False, f"HTTP {response.status_code}", 0
        
        # å°è¯•è§£æJSON
        try:
            data = response.json()
            if isinstance(data, dict) or isinstance(data, list):
                item_count = len(data) if isinstance(data, list) else 1
                return True, f"æˆåŠŸè·å–æ•°æ® ({elapsed:.1f}ç§’)", item_count
            else:
                return False, "å“åº”æ ¼å¼æ— æ•ˆ", 0
        except:
            # å¦‚æœä¸æ˜¯JSONï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
            if response.text:
                return True, f"æˆåŠŸè·å–æ–‡æœ¬å†…å®¹ ({elapsed:.1f}ç§’)", 1
            else:
                return False, "å“åº”å†…å®¹ä¸ºç©º", 0
                
    except requests.exceptions.Timeout:
        return False, "è¯·æ±‚è¶…æ—¶", 0
    except requests.exceptions.ConnectionError:
        return False, "è¿æ¥é”™è¯¯", 0
    except Exception as e:
        return False, f"å¼‚å¸¸: {str(e)}", 0

def get_news_sources() -> List[Dict]:
    """è·å–æ–°é—»æºåˆ—è¡¨"""
    return [
        # å›½å†…æ–°é—»åª’ä½“
        {
            'name': 'æ–°æµªæ–°é—»',
            'type': 'rss',
            'url': 'http://rss.sina.com.cn/news/marquee/ddt.xml',
            'category': 'å›½å†…åª’ä½“'
        },
        {
            'name': 'ç½‘æ˜“æ–°é—»',
            'type': 'rss', 
            'url': 'http://news.163.com/special/00011K6L/rss_newsattitude.xml',
            'category': 'å›½å†…åª’ä½“'
        },
        {
            'name': 'å‡¤å‡°æ–°é—»',
            'type': 'rss',
            'url': 'https://news.ifeng.com/rss/ifengnews.xml',
            'category': 'å›½å†…åª’ä½“'
        },
        {
            'name': 'æ¾æ¹ƒæ–°é—»',
            'type': 'rss',
            'url': 'https://www.thepaper.cn/rss_hot.jsp',
            'category': 'å›½å†…åª’ä½“'
        },
        {
            'name': 'ä»Šæ—¥å¤´æ¡çƒ­æ¦œ',
            'type': 'api',
            'url': 'https://www.toutiao.com/hot-event/hot-board/',
            'category': 'ç¤¾äº¤åª’ä½“'
        },
        
        # å›½é™…æ–°é—»åª’ä½“
        {
            'name': 'BBCä¸­æ–‡ç½‘',
            'type': 'rss',
            'url': 'https://www.bbc.com/zhongwen/simp/index.xml',
            'category': 'å›½é™…åª’ä½“'
        },
        {
            'name': 'BBC World',
            'type': 'rss',
            'url': 'http://feeds.bbci.co.uk/news/world/rss.xml',
            'category': 'å›½é™…åª’ä½“'
        },
        {
            'name': 'CNNå›½é™…ç‰ˆ',
            'type': 'rss',
            'url': 'http://rss.cnn.com/rss/edition.rss',
            'category': 'å›½é™…åª’ä½“'
        },
        {
            'name': 'é‡‘èæ—¶æŠ¥ä¸­æ–‡ç½‘',
            'type': 'rss',
            'url': 'https://www.ftchinese.com/rss/feed',
            'category': 'å›½é™…åª’ä½“'
        },
        {
            'name': 'æ—¥ç»äºšæ´²',
            'type': 'rss',
            'url': 'https://asia.nikkei.com/rss/feed/nar',
            'category': 'å›½é™…åª’ä½“'
        },
        {
            'name': 'å—åæ—©æŠ¥',
            'type': 'rss',
            'url': 'https://www.scmp.com/rss/91/feed',
            'category': 'å›½é™…åª’ä½“'
        }
    ]

def test_all_sources():
    """æµ‹è¯•æ‰€æœ‰æ–°é—»æº"""
    print("ğŸ” æ–°é—»æºæœ‰æ•ˆæ€§æµ‹è¯•")
    print("=" * 80)
    
    sources = get_news_sources()
    results = []
    
    for source in sources:
        print(f"æµ‹è¯•: {source['name']} ({source['category']})")
        print(f"  URL: {source['url']}")
        
        if source['type'] == 'rss':
            success, message, count = test_rss_source(source['url'], source['name'])
        else:  # api
            success, message, count = test_api_source(source['url'], source['name'])
        
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"  çŠ¶æ€: {status}")
        print(f"  ä¿¡æ¯: {message}")
        print(f"  æ•°é‡: {count}")
        print()
        
        results.append({
            'name': source['name'],
            'category': source['category'],
            'type': source['type'],
            'url': source['url'],
            'success': success,
            'message': message,
            'count': count
        })
    
    # ç»Ÿè®¡ç»“æœ
    print("=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
    print()
    
    total = len(results)
    successful = sum(1 for r in results if r['success'])
    failed = total - successful
    
    print(f"æ€»è®¡: {total} ä¸ªæ–°é—»æº")
    print(f"æˆåŠŸ: {successful} ä¸ª ({successful/total*100:.1f}%)")
    print(f"å¤±è´¥: {failed} ä¸ª ({failed/total*100:.1f}%)")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    categories = {}
    for r in results:
        cat = r['category']
        if cat not in categories:
            categories[cat] = {'total': 0, 'success': 0}
        categories[cat]['total'] += 1
        if r['success']:
            categories[cat]['success'] += 1
    
    print("\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for cat, stats in categories.items():
        success_rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
        print(f"  {cat}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
    
    # æ˜¾ç¤ºå¤±è´¥çš„æº
    failed_sources = [r for r in results if not r['success']]
    if failed_sources:
        print("\nâŒ å¤±è´¥çš„æ–°é—»æº:")
        for r in failed_sources:
            print(f"  â€¢ {r['name']}: {r['message']}")
    
    # æ˜¾ç¤ºæˆåŠŸçš„æºï¼ˆæŒ‰æ–‡ç« æ•°é‡æ’åºï¼‰
    successful_sources = [r for r in results if r['success']]
    if successful_sources:
        print("\nâœ… æˆåŠŸçš„æ–°é—»æº (æŒ‰æ–‡ç« æ•°é‡æ’åº):")
        for r in sorted(successful_sources, key=lambda x: x['count'], reverse=True):
            print(f"  â€¢ {r['name']}: {r['count']} ç¯‡æ–‡ç«  - {r['message']}")
    
    # ä¿å­˜ç»“æœ
    with open('news_source_test_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total': total,
            'successful': successful,
            'failed': failed,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: news_source_test_results.json")
    
    return results

def suggest_new_sources():
    """å»ºè®®æ–°çš„æ–°é—»æº"""
    print("\n" + "=" * 80)
    print("ğŸ’¡ å»ºè®®çš„æ–°æ–°é—»æº")
    print()
    
    new_sources = [
        # å›½é™…é«˜è´¨é‡åª’ä½“
        {
            'name': 'ç»æµå­¦äºº (The Economist)',
            'type': 'rss',
            'url': 'https://www.economist.com/rss',
            'category': 'å›½é™…åª’ä½“',
            'description': 'å…¨çƒçŸ¥åç»æµå’Œæ”¿æ²»æ‚å¿—'
        },
        {
            'name': 'çº½çº¦å®¢ (The New Yorker)',
            'type': 'rss', 
            'url': 'https://www.newyorker.com/rss',
            'category': 'å›½é™…åª’ä½“',
            'description': 'ç¾å›½çŸ¥åæ–‡åŒ–å’Œæ—¶äº‹æ‚å¿—'
        },
        {
            'name': 'åå°”è¡—æ—¥æŠ¥ (Wall Street Journal)',
            'type': 'rss',
            'url': 'https://feeds.a.dj.com/rss/RSSWorldNews.xml',
            'category': 'å›½é™…åª’ä½“',
            'description': 'å…¨çƒçŸ¥åè´¢ç»æŠ¥çº¸'
        },
        {
            'name': 'è·¯é€ç¤¾ (Reuters)',
            'type': 'rss',
            'url': 'http://feeds.reuters.com/reuters/topNews',
            'category': 'å›½é™…åª’ä½“',
            'description': 'å›½é™…æ–°é—»é€šè®¯ç¤¾'
        },
        {
            'name': 'ç¾è”ç¤¾ (Associated Press)',
            'type': 'rss',
            'url': 'https://apnews.com/rss',
            'category': 'å›½é™…åª’ä½“',
            'description': 'ç¾å›½æ–°é—»é€šè®¯ç¤¾'
        },
        
        # ç§‘æŠ€åª’ä½“
        {
            'name': 'TechCrunch',
            'type': 'rss',
            'url': 'http://feeds.feedburner.com/TechCrunch/',
            'category': 'ç§‘æŠ€åª’ä½“',
            'description': 'å…¨çƒçŸ¥åç§‘æŠ€æ–°é—»åª’ä½“'
        },
        {
            'name': 'Wired',
            'type': 'rss',
            'url': 'https://www.wired.com/feed/rss',
            'category': 'ç§‘æŠ€åª’ä½“',
            'description': 'ç§‘æŠ€å’Œæ–‡åŒ–æ‚å¿—'
        },
        {
            'name': 'Ars Technica',
            'type': 'rss',
            'url': 'http://feeds.arstechnica.com/arstechnica/index',
            'category': 'ç§‘æŠ€åª’ä½“',
            'description': 'æ·±åº¦ç§‘æŠ€æ–°é—»å’Œåˆ†æ'
        },
        
        # ä¸­æ–‡ä¼˜è´¨åª’ä½“
        {
            'name': 'è´¢æ–°ç½‘',
            'type': 'rss',
            'url': 'https://rss.caixin.com/',
            'category': 'å›½å†…åª’ä½“',
            'description': 'ä¸­å›½çŸ¥åè´¢ç»åª’ä½“'
        },
        {
            'name': 'è™å—…',
            'type': 'rss',
            'url': 'https://www.huxiu.com/rss/0.xml',
            'category': 'å›½å†…åª’ä½“',
            'description': 'ä¸­å›½ç§‘æŠ€å’Œå•†ä¸šåª’ä½“'
        },
        {
            'name': '36æ°ª',
            'type': 'rss',
            'url': 'https://www.36kr.com/feed',
            'category': 'å›½å†…åª’ä½“',
            'description': 'ä¸­å›½åˆ›ä¸šå’ŒæŠ•èµ„åª’ä½“'
        },
        
        # åŒºåŸŸåª’ä½“
        {
            'name': 'æœæ—¥æ–°é—» (Asahi Shimbun)',
            'type': 'rss',
            'url': 'https://www.asahi.com/rss/index.rdf',
            'category': 'åŒºåŸŸåª’ä½“',
            'description': 'æ—¥æœ¬çŸ¥åæŠ¥çº¸'
        },
        {
            'name': 'éŸ©å›½ä¸­å¤®æ—¥æŠ¥ (JoongAng Ilbo)',
            'type': 'rss',
            'url': 'https://rss.joins.com/joins_news_list.xml',
            'category': 'åŒºåŸŸåª’ä½“',
            'description': 'éŸ©å›½çŸ¥åæŠ¥çº¸'
        },
        {
            'name': 'æµ·å³¡æ—¶æŠ¥ (The Straits Times)',
            'type': 'rss',
            'url': 'https://www.straitstimes.com/news/rss.xml',
            'category': 'åŒºåŸŸåª’ä½“',
            'description': 'æ–°åŠ å¡ä¸»è¦è‹±æ–‡æŠ¥çº¸'
        }
    ]
    
    print("æ¨èæ·»åŠ ä»¥ä¸‹é«˜è´¨é‡æ–°é—»æº:")
    for i, source in enumerate(new_sources, 1):
        print(f"{i}. {source['name']} ({source['category']})")
        print(f"   URL: {source['url']}")
        print(f"   æè¿°: {source['description']}")
        print()
    
    return new_sources

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“° æ–°é—»æºæ£€æŸ¥å’Œä¼˜åŒ–å·¥å…·")
    print("=" * 80)
    
    # æµ‹è¯•ç°æœ‰æº
    results = test_all_sources()
    
    # å»ºè®®æ–°æº
    new_sources = suggest_new_sources()
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    print("=" * 80)
    print("ğŸ”§ ä¼˜åŒ–å»ºè®®")
    print()
    
    # æ‰¾å‡ºå¤±è´¥çš„æº
    failed_sources = [r for r in results if not r['success']]
    if failed_sources:
        print("1. å»ºè®®ç§»é™¤ä»¥ä¸‹å¤±æ•ˆçš„æ–°é—»æº:")
        for r in failed_sources:
            print(f"   â€¢ {r['name']} - {r['message']}")
        print()
    
    # æ‰¾å‡ºæ–‡ç« æ•°é‡å°‘çš„æº
    low_count_sources = [r for r in results if r['success'] and r['count'] < 5]
    if low_count_sources:
        print("2. ä»¥ä¸‹æ–°é—»æºæ–‡ç« æ•°é‡è¾ƒå°‘ (<5ç¯‡):")
        for r in low_count_sources:
            print(f"   â€¢ {r['name']} - ä»… {r['count']} ç¯‡æ–‡ç« ")
        print("   è€ƒè™‘æ›¿æ¢æˆ–ä¿ç•™ä½œä¸ºè¡¥å……")
        print()
    
    # å»ºè®®æ·»åŠ çš„æ–°æº
    print("3. å»ºè®®æ·»åŠ ä»¥ä¸‹é«˜è´¨é‡æ–°é—»æºä»¥ä¸°å¯Œå†…å®¹:")
    print("   â€¢ ç»æµå­¦äºº (The Economist) - å…¨çƒçŸ¥åç»æµå’Œæ”¿æ²»æ‚å¿—")
    print("   â€¢ çº½çº¦å®¢ (The New Yorker) - ç¾å›½æ–‡åŒ–å’Œæ—¶äº‹æ‚å¿—")
    print("   â€¢ åå°”è¡—æ—¥æŠ¥ (Wall Street Journal) - å…¨çƒçŸ¥åè´¢ç»æŠ¥çº¸")
    print("   â€¢ è·¯é€ç¤¾ (Reuters) - å›½é™…æ–°é—»é€šè®¯ç¤¾")
    print("   â€¢ TechCrunch - å…¨çƒçŸ¥åç§‘æŠ€æ–°é—»åª’ä½“")
    print()
    
    print("âœ… æµ‹è¯•å®Œæˆï¼Œè¯·æ ¹æ®ç»“æœä¼˜åŒ–æ–°é—»æºé…ç½®")

if __name__ == "__main__":
    main()