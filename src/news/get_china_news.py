#!/usr/bin/env python3
"""
è·å–ä¸­å›½é—¨æˆ·ç½‘ç«™æœ€æ–°æ¶ˆæ¯ç»Ÿè®¡
"""

import requests
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
import xml.etree.ElementTree as ET
from dataclasses import dataclass
import re

@dataclass
class NewsItem:
    """æ–°é—»æ¡ç›®"""
    title: str
    source: str
    url: str
    time: str
    category: str = ""
    summary: str = ""

class NewsFetcher:
    """æ–°é—»è·å–å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        })
    
    def fetch_sina_news(self) -> List[NewsItem]:
        """è·å–æ–°æµªæ–°é—»"""
        news_items = []
        try:
            # æ–°æµªæ–°é—»RSS
            rss_urls = [
                "http://rss.sina.com.cn/news/marquee/ddt.xml",  # æ»šåŠ¨æ–°é—»
                "http://rss.sina.com.cn/news/china/focus15.xml",  # å›½å†…ç„¦ç‚¹
            ]
            
            for rss_url in rss_urls:
                try:
                    response = self.session.get(rss_url, timeout=10)
                    if response.status_code == 200:
                        root = ET.fromstring(response.content)
                        for item in root.findall(".//item"):
                            title = item.find("title").text if item.find("title") is not None else ""
                            link = item.find("link").text if item.find("link") is not None else ""
                            pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""
                            
                            if title and link:
                                news_items.append(NewsItem(
                                    title=title,
                                    source="æ–°æµªæ–°é—»",
                                    url=link,
                                    time=pub_date,
                                    category="ç»¼åˆ"
                                ))
                except:
                    continue
                    
        except Exception as e:
            print(f"è·å–æ–°æµªæ–°é—»é”™è¯¯: {e}")
        
        return news_items[:10]  # è¿”å›å‰10æ¡
    
    def fetch_tencent_news(self) -> List[NewsItem]:
        """è·å–è…¾è®¯æ–°é—»"""
        news_items = []
        try:
            # å°è¯•è·å–è…¾è®¯æ–°é—»API
            api_url = "https://r.inews.qq.com/gw/event/pc_hot_ranking_list"
            params = {
                "ids": "",
                "page": 0,
                "type": 1
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get("ret") == 0:
                    news_list = data.get("idlist", [{}])[0].get("newslist", [])
                    for news in news_list[:10]:
                        title = news.get("title", "")
                        url = f"https://new.qq.com/rain/a/{news.get('id', '')}"
                        time_str = news.get("time", "")
                        
                        if title:
                            news_items.append(NewsItem(
                                title=title,
                                source="è…¾è®¯æ–°é—»",
                                url=url,
                                time=time_str,
                                category="çƒ­ç‚¹"
                            ))
                            
        except Exception as e:
            print(f"è·å–è…¾è®¯æ–°é—»é”™è¯¯: {e}")
        
        return news_items
    
    def fetch_netease_news(self) -> List[NewsItem]:
        """è·å–ç½‘æ˜“æ–°é—»"""
        news_items = []
        try:
            # ç½‘æ˜“æ–°é—»çƒ­æ¦œAPI
            api_url = "https://gw.m.163.com/nc/api/v1/hot/hotList"
            params = {
                "page": 1,
                "size": 20,
                "sp": "news",
                "post": 1
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get("data", [])
                for item in items[:10]:
                    title = item.get("title", "")
                    docid = item.get("docid", "")
                    if title and docid:
                        url = f"https://www.163.com/dy/article/{docid}.html"
                        news_items.append(NewsItem(
                            title=title,
                            source="ç½‘æ˜“æ–°é—»",
                            url=url,
                            time=datetime.now().strftime("%Y-%m-%d %H:%M"),
                            category="çƒ­ç‚¹"
                        ))
                        
        except Exception as e:
            print(f"è·å–ç½‘æ˜“æ–°é—»é”™è¯¯: {e}")
        
        return news_items
    
    def fetch_toutiao_news(self) -> List[NewsItem]:
        """è·å–ä»Šæ—¥å¤´æ¡æ–°é—»"""
        news_items = []
        try:
            # ä»Šæ—¥å¤´æ¡çƒ­æ¦œ
            api_url = "https://www.toutiao.com/hot-event/hot-board/"
            params = {
                "origin": "toutiao_pc"
            }
            
            response = self.session.get(api_url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get("data", [])
                for item in items[:10]:
                    title = item.get("Title", "")
                    url = item.get("Url", "")
                    hot_value = item.get("HotValue", 0)
                    
                    if title and url:
                        news_items.append(NewsItem(
                            title=f"{title} ({hot_value}çƒ­åº¦)",
                            source="ä»Šæ—¥å¤´æ¡",
                            url=url,
                            time=datetime.now().strftime("%Y-%m-%d %H:%M"),
                            category="çƒ­æ¦œ"
                        ))
                        
        except Exception as e:
            print(f"è·å–ä»Šæ—¥å¤´æ¡é”™è¯¯: {e}")
        
        return news_items
    
    def fetch_ifeng_news(self) -> List[NewsItem]:
        """è·å–å‡¤å‡°æ–°é—»"""
        news_items = []
        try:
            # å‡¤å‡°æ–°é—»RSS
            rss_url = "https://news.ifeng.com/rss/ifengnews.xml"
            
            response = self.session.get(rss_url, timeout=10)
            if response.status_code == 200:
                root = ET.fromstring(response.content)
                for item in root.findall(".//item")[:10]:
                    title = item.find("title").text if item.find("title") is not None else ""
                    link = item.find("link").text if item.find("link") is not None else ""
                    pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""
                    
                    if title and link:
                        news_items.append(NewsItem(
                            title=title,
                            source="å‡¤å‡°æ–°é—»",
                            url=link,
                            time=pub_date,
                            category="ç»¼åˆ"
                        ))
                        
        except Exception as e:
            print(f"è·å–å‡¤å‡°æ–°é—»é”™è¯¯: {e}")
        
        return news_items

class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""
    
    @staticmethod
    def analyze_news(news_items: List[NewsItem]) -> Dict:
        """åˆ†ææ–°é—»æ•°æ®"""
        if not news_items:
            return {}
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_stats = {}
        for news in news_items:
            source = news.source
            source_stats[source] = source_stats.get(source, 0) + 1
        
        # æå–å…³é”®è¯
        keywords = NewsAnalyzer.extract_keywords(news_items)
        
        # çƒ­é—¨è¯é¢˜
        hot_topics = NewsAnalyzer.identify_hot_topics(news_items)
        
        return {
            "total_news": len(news_items),
            "sources": source_stats,
            "top_keywords": keywords[:10],
            "hot_topics": hot_topics,
            "latest_time": max([n.time for n in news_items if n.time], default=""),
            "sources_count": len(source_stats)
        }
    
    @staticmethod
    def extract_keywords(news_items: List[NewsItem]) -> List[str]:
        """æå–å…³é”®è¯"""
        all_titles = " ".join([n.title for n in news_items])
        
        # å¸¸è§æ–°é—»å…³é”®è¯
        common_keywords = [
            "ç–«æƒ…", "ç»æµ", "ç§‘æŠ€", "æ”¿æ²»", "å›½é™…", "ç¤¾ä¼š", "è´¢ç»", 
            "ä½“è‚²", "å¨±ä¹", "æ•™è‚²", "å¥åº·", "ç¯å¢ƒ", "èƒ½æº", "äº¤é€š"
        ]
        
        keywords = []
        for keyword in common_keywords:
            if keyword in all_titles:
                keywords.append(keyword)
        
        return keywords
    
    @staticmethod
    def identify_hot_topics(news_items: List[NewsItem]) -> List[Dict]:
        """è¯†åˆ«çƒ­é—¨è¯é¢˜"""
        topics = []
        
        # æŒ‰å…³é”®è¯åˆ†ç»„
        keyword_groups = {}
        for news in news_items:
            for keyword in ["ç–«æƒ…", "ç»æµ", "ç§‘æŠ€", "å›½é™…"]:
                if keyword in news.title:
                    if keyword not in keyword_groups:
                        keyword_groups[keyword] = []
                    keyword_groups[keyword].append(news)
        
        for keyword, items in keyword_groups.items():
            if len(items) >= 2:  # è‡³å°‘2æ¡ç›¸å…³æ–°é—»
                topics.append({
                    "topic": keyword,
                    "count": len(items),
                    "sources": list(set([item.source for item in items])),
                    "sample_titles": [item.title[:30] + "..." for item in items[:3]]
                })
        
        return topics
    
    @staticmethod
    def generate_report(news_items: List[NewsItem], analysis: Dict) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        report = []
        report.append("# ä¸­å›½é—¨æˆ·ç½‘ç«™æœ€æ–°æ¶ˆæ¯ç»Ÿè®¡")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}")
        report.append(f"ç»Ÿè®¡æ–°é—»æ•°é‡: {analysis.get('total_news', 0)}æ¡")
        report.append(f"è¦†ç›–æ–°é—»æº: {analysis.get('sources_count', 0)}ä¸ª")
        report.append("")
        
        # æ–°é—»æºç»Ÿè®¡
        report.append("## ğŸ“Š æ–°é—»æºç»Ÿè®¡")
        source_stats = analysis.get("sources", {})
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            report.append(f"- **{source}**: {count}æ¡æ–°é—»")
        report.append("")
        
        # çƒ­é—¨è¯é¢˜
        report.append("## ğŸ”¥ çƒ­é—¨è¯é¢˜")
        hot_topics = analysis.get("hot_topics", [])
        if hot_topics:
            for topic in hot_topics[:5]:
                report.append(f"### {topic['topic']} ({topic['count']}æ¡)")
                report.append(f"æ¶‰åŠåª’ä½“: {', '.join(topic['sources'])}")
                for title in topic['sample_titles']:
                    report.append(f"- {title}")
                report.append("")
        else:
            report.append("æš‚æ— æ˜¾è‘—çƒ­é—¨è¯é¢˜")
            report.append("")
        
        # å…³é”®è¯
        report.append("## ğŸ”‘ å…³é”®è¯åˆ†æ")
        keywords = analysis.get("top_keywords", [])
        if keywords:
            report.append(" ".join([f"`{kw}`" for kw in keywords]))
        else:
            report.append("å…³é”®è¯æå–ä¸­...")
        report.append("")
        
        # æ–°é—»åˆ—è¡¨
        report.append("## ğŸ“° æœ€æ–°æ–°é—»æ‘˜è¦")
        
        # æŒ‰æ¥æºåˆ†ç»„æ˜¾ç¤º
        sources_news = {}
        for news in news_items:
            if news.source not in sources_news:
                sources_news[news.source] = []
            sources_news[news.source].append(news)
        
        for source, items in sources_news.items():
            report.append(f"### {source}")
            for i, news in enumerate(items[:5], 1):
                time_str = news.time[:16] if news.time else "æ—¶é—´æœªçŸ¥"
                report.append(f"{i}. **{news.title}**")
                report.append(f"   æ—¶é—´: {time_str}")
                if news.url:
                    report.append(f"   é“¾æ¥: {news.url}")
                report.append("")
        
        # æ€»ç»“
        report.append("## ğŸ“ˆ æ€»ç»“")
        report.append(f"1. **æ–°é—»æ€»é‡**: å…±æ”¶é›†åˆ° {analysis.get('total_news', 0)} æ¡æ–°é—»")
        report.append(f"2. **è¦†ç›–å¹¿åº¦**: æ¥è‡ª {analysis.get('sources_count', 0)} ä¸ªä¸»è¦æ–°é—»æº")
        report.append(f"3. **æ—¶æ•ˆæ€§**: æœ€æ–°æ–°é—»æ—¶é—´ {analysis.get('latest_time', 'æœªçŸ¥')}")
        report.append(f"4. **è¯é¢˜åˆ†å¸ƒ**: æ¶µç›– {len(hot_topics)} ä¸ªä¸»è¦è¯é¢˜é¢†åŸŸ")
        report.append("")
        report.append("> æ•°æ®æ¥æº: å„é—¨æˆ·ç½‘ç«™å…¬å¼€APIå’ŒRSSæº")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è·å–ä¸­å›½é—¨æˆ·ç½‘ç«™æœ€æ–°æ¶ˆæ¯...")
    print("="*60)
    
    fetcher = NewsFetcher()
    all_news = []
    
    # è·å–å„å¹³å°æ–°é—»
    print("ğŸ“¡ è·å–æ–°æµªæ–°é—»...")
    all_news.extend(fetcher.fetch_sina_news())
    
    print("ğŸ“¡ è·å–è…¾è®¯æ–°é—»...")
    all_news.extend(fetcher.fetch_tencent_news())
    
    print("ğŸ“¡ è·å–ç½‘æ˜“æ–°é—»...")
    all_news.extend(fetcher.fetch_netease_news())
    
    print("ğŸ“¡ è·å–ä»Šæ—¥å¤´æ¡...")
    all_news.extend(fetcher.fetch_toutiao_news())
    
    print("ğŸ“¡ è·å–å‡¤å‡°æ–°é—»...")
    all_news.extend(fetcher.fetch_ifeng_news())
    
    print("="*60)
    print(f"âœ… å…±è·å– {len(all_news)} æ¡æ–°é—»")
    
    if not all_news:
        print("âŒ æœªèƒ½è·å–åˆ°æ–°é—»æ•°æ®")
        return
    
    # åˆ†ææ–°é—»
    print("ğŸ“Š åˆ†ææ–°é—»æ•°æ®...")
    analyzer = NewsAnalyzer()
    analysis = analyzer.analyze_news(all_news)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“ ç”ŸæˆæŠ¥å‘Š...")
    report = analyzer.generate_report(all_news, analysis)
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"china_news_report_{timestamp}.md"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    
    # æ˜¾ç¤ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
    print(f"  æ–°é—»æ€»æ•°: {analysis.get('total_news', 0)}æ¡")
    
    source_stats = analysis.get("sources", {})
    print("  æ–°é—»æºåˆ†å¸ƒ:")
    for source, count in source_stats.items():
        print(f"    {source}: {count}æ¡")
    
    print(f"  çƒ­é—¨è¯é¢˜: {len(analysis.get('hot_topics', []))}ä¸ª")
    print(f"  å…³é”®è¯: {', '.join(analysis.get('top_keywords', [])[:5])}")
    print("="*60)
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ–°é—»
    print("\nğŸ”¥ éƒ¨åˆ†çƒ­é—¨æ–°é—»:")
    for i, news in enumerate(all_news[:5], 1):
        print(f"{i}. [{news.source}] {news.title[:50]}...")

if __name__ == "__main__":
    main()