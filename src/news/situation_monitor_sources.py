#!/usr/bin/env python3
"""
å€Ÿé‰´ situation-monitor ç†å¿µçš„å¢å¼ºæ•°æ®æº
ä¸“æ³¨äºæŠ€æœ¯ç›‘æ§ã€å®‰å…¨ã€DevOpsã€æ€§èƒ½ç­‰ä¸“ä¸šé¢†åŸŸ
"""

from typing import List, Dict, Any, Optional
import requests
import xml.etree.ElementTree as ET
import json
import re
import time
from datetime import datetime, timedelta

class SituationMonitorNewsSources:
    """situation-monitor é£æ ¼çš„æ•°æ®æºé›†åˆ"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7"
        })
    
    def get_all_sources(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰ situation-monitor é£æ ¼çš„æ•°æ®æº
        
        Returns:
            æ•°æ®æºé…ç½®åˆ—è¡¨
        """
        return [
            # ==================== æŠ€æœ¯ç›‘æ§é¢†åŸŸ ====================
            {
                'name': 'Grafana Labs Blog',
                'type': 'rss',
                'url': 'https://grafana.com/blog/index.xml',
                'category': 'æŠ€æœ¯ç›‘æ§',
                'description': 'Grafanaç›‘æ§ã€å¯è§†åŒ–æœ€æ–°åŠ¨æ€',
                'tags': ['ç›‘æ§', 'å¯è§†åŒ–', 'grafana', 'devops']
            },
            {
                'name': 'Prometheus Blog',
                'type': 'rss',
                'url': 'https://prometheus.io/blog/feed.xml',
                'category': 'æŠ€æœ¯ç›‘æ§',
                'description': 'Prometheusç›‘æ§ç³»ç»Ÿæœ€æ–°æ›´æ–°',
                'tags': ['ç›‘æ§', 'prometheus', 'metrics', 'devops']
            },
            {
                'name': 'Datadog Blog',
                'type': 'rss',
                'url': 'https://www.datadoghq.com/blog/feed/',
                'category': 'æŠ€æœ¯ç›‘æ§',
                'description': 'Datadogç›‘æ§å’Œå¯è§‚æµ‹æ€§å¹³å°åšå®¢',
                'tags': ['ç›‘æ§', 'å¯è§‚æµ‹æ€§', 'datadog', 'apm']
            },
            
            # ==================== å®‰å…¨ç›‘æ§é¢†åŸŸ ====================
            {
                'name': 'CISA Alerts',
                'type': 'rss',
                'url': 'https://www.cisa.gov/news/rss.xml',
                'category': 'å®‰å…¨ç›‘æ§',
                'description': 'ç¾å›½ç½‘ç»œå®‰å…¨ä¸åŸºç¡€è®¾æ–½å®‰å…¨å±€è­¦æŠ¥',
                'tags': ['å®‰å…¨', 'å¨èƒæƒ…æŠ¥', 'cisa', 'ç½‘ç»œå®‰å…¨']
            },
            {
                'name': 'KrebsOnSecurity',
                'type': 'rss',
                'url': 'https://krebsonsecurity.com/feed/',
                'category': 'å®‰å…¨ç›‘æ§',
                'description': 'ç½‘ç»œå®‰å…¨è°ƒæŸ¥å’Œæ–°é—»',
                'tags': ['å®‰å…¨', 'é»‘å®¢', 'è°ƒæŸ¥', 'ç½‘ç»œå®‰å…¨']
            },
            {
                'name': 'Threatpost',
                'type': 'rss',
                'url': 'https://threatpost.com/feed/',
                'category': 'å®‰å…¨ç›‘æ§',
                'description': 'ç½‘ç»œå®‰å…¨æ–°é—»å’Œåˆ†æ',
                'tags': ['å®‰å…¨', 'å¨èƒ', 'æ¼æ´', 'æ¶æ„è½¯ä»¶']
            },
            {
                'name': 'SecurityWeek RSS',
                'type': 'rss',
                'url': 'https://feeds.feedburner.com/securityweek',
                'category': 'å®‰å…¨ç›‘æ§',
                'description': 'ç½‘ç»œå®‰å…¨æ–°é—»å’Œæ´å¯Ÿ',
                'tags': ['å®‰å…¨', 'ç½‘ç»œå®‰å…¨', 'ä¼ä¸šå®‰å…¨']
            },
            
            # ==================== DevOps å’Œ SRE ====================
            {
                'name': 'Google SRE Blog',
                'type': 'rss',
                'url': 'https://sre.google/feed.xml',
                'category': 'DevOps/SRE',
                'description': 'Googleç«™ç‚¹å¯é æ€§å·¥ç¨‹åšå®¢',
                'tags': ['sre', 'reliability', 'google', 'devops']
            },
            {
                'name': 'Netflix TechBlog',
                'type': 'rss',
                'url': 'https://netflixtechblog.com/feed',
                'category': 'DevOps/SRE',
                'description': 'NetflixæŠ€æœ¯å·¥ç¨‹åšå®¢',
                'tags': ['netflix', 'å¾®æœåŠ¡', 'å¯æ‰©å±•æ€§', 'sre']
            },
            {
                'name': 'Uber Engineering Blog',
                'type': 'rss',
                'url': 'https://eng.uber.com/feed/',
                'category': 'DevOps/SRE',
                'description': 'Uberå·¥ç¨‹åšå®¢',
                'tags': ['uber', 'å·¥ç¨‹', 'å¯æ‰©å±•æ€§', 'æ¶æ„']
            },
            
            # ==================== æ€§èƒ½ç›‘æ§å’Œå¯è§‚æµ‹æ€§ ====================
            {
                'name': 'New Relic Blog',
                'type': 'rss',
                'url': 'https://newrelic.com/blog/feed',
                'category': 'æ€§èƒ½ç›‘æ§',
                'description': 'New Relicå¯è§‚æµ‹æ€§å’ŒAPMåšå®¢',
                'tags': ['apm', 'å¯è§‚æµ‹æ€§', 'æ€§èƒ½', 'newrelic']
            },
            {
                'name': 'LightStep Blog',
                'type': 'rss',
                'url': 'https://lightstep.com/blog/feed/',
                'category': 'æ€§èƒ½ç›‘æ§',
                'description': 'åˆ†å¸ƒå¼è¿½è¸ªå’Œå¯è§‚æµ‹æ€§',
                'tags': ['tracing', 'å¯è§‚æµ‹æ€§', 'å¾®æœåŠ¡', 'lightstep']
            },
            
            # ==================== å¼€æºç›‘æ§é¡¹ç›® ====================
            {
                'name': 'OpenTelemetry Blog',
                'type': 'rss',
                'url': 'https://opentelemetry.io/blog/index.xml',
                'category': 'å¼€æºç›‘æ§',
                'description': 'OpenTelemetryå¯è§‚æµ‹æ€§æ¡†æ¶',
                'tags': ['opentelemetry', 'å¯è§‚æµ‹æ€§', 'å¼€æº', 'æ ‡å‡†']
            },
            {
                'name': 'Jaeger Tracing',
                'type': 'rss',
                'url': 'https://www.jaegertracing.io/blog/index.xml',
                'category': 'å¼€æºç›‘æ§',
                'description': 'Jaegeråˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ',
                'tags': ['jaeger', 'tracing', 'åˆ†å¸ƒå¼', 'ç›‘æ§']
            },
            
            # ==================== åŸºç¡€è®¾æ–½ç›‘æ§ ====================
            {
                'name': 'Kubernetes Blog',
                'type': 'rss',
                'url': 'https://kubernetes.io/feed.xml',
                'category': 'åŸºç¡€è®¾æ–½ç›‘æ§',
                'description': 'Kuberneteså®˜æ–¹åšå®¢',
                'tags': ['kubernetes', 'å®¹å™¨', 'ç¼–æ’', 'äº‘åŸç”Ÿ']
            },
            {
                'name': 'Docker Blog',
                'type': 'rss',
                'url': 'https://www.docker.com/blog/feed/',
                'category': 'åŸºç¡€è®¾æ–½ç›‘æ§',
                'description': 'Dockerå®¹å™¨æŠ€æœ¯åšå®¢',
                'tags': ['docker', 'å®¹å™¨', 'devops', 'äº‘åŸç”Ÿ']
            },
            
            # ==================== é‡‘èç§‘æŠ€ç›‘æ§ ====================
            {
                'name': 'Finextra',
                'type': 'rss',
                'url': 'https://www.finextra.com/rss/feeds.aspx',
                'category': 'é‡‘èç§‘æŠ€ç›‘æ§',
                'description': 'é‡‘èç§‘æŠ€æ–°é—»å’Œåˆ›æ–°',
                'tags': ['fintech', 'é‡‘èç§‘æŠ€', 'é“¶è¡Œ', 'æ”¯ä»˜']
            },
            {
                'name': 'The Banker',
                'type': 'rss',
                'url': 'https://www.thebanker.com/RSS',
                'category': 'é‡‘èç§‘æŠ€ç›‘æ§',
                'description': 'å›½é™…é“¶è¡Œä¸šæ–°é—»å’Œåˆ†æ',
                'tags': ['é“¶è¡Œ', 'é‡‘è', 'ç›‘ç®¡', 'é£é™©']
            },
            
            # ==================== æ•°æ®ç›‘æ§å’Œåˆ†æ ====================
            {
                'name': 'Apache Kafka Blog',
                'type': 'rss',
                'url': 'https://kafka.apache.org/blog/feed.xml',
                'category': 'æ•°æ®ç›‘æ§',
                'description': 'Apache Kafkaæµå¤„ç†å¹³å°',
                'tags': ['kafka', 'æµå¤„ç†', 'å¤§æ•°æ®', 'å®æ—¶']
            },
            {
                'name': 'Elastic Blog',
                'type': 'rss',
                'url': 'https://www.elastic.co/blog/feed',
                'category': 'æ•°æ®ç›‘æ§',
                'description': 'Elasticsearchã€Logstashã€Kibanaåšå®¢',
                'tags': ['elasticsearch', 'elk', 'æ—¥å¿—', 'æœç´¢']
            }
        ]
    
    def fetch_articles_from_source(self, source: Dict[str, Any], limit: int = 5) -> List[Dict[str, Any]]:
        """
        ä»å•ä¸ªæ•°æ®æºè·å–æ–‡ç« 
        
        Args:
            source: æ•°æ®æºé…ç½®
            limit: æœ€å¤§æ–‡ç« æ•°é‡
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        articles = []
        
        try:
            if source['type'] == 'rss':
                response = self.session.get(source['url'], timeout=10)
                if response.status_code == 200:
                    # å°è¯•è§£æRSS/Atom
                    root = ET.fromstring(response.content)
                    
                    # RSSæ ¼å¼
                    items = root.findall('.//item') or root.findall('.//entry')
                    
                    for i, item in enumerate(items[:limit]):
                        try:
                            # æå–æ ‡é¢˜
                            title_elem = item.find('title') or item.find('{http://www.w3.org/2005/Atom}title')
                            title = title_elem.text if title_elem is not None else 'æ— æ ‡é¢˜'
                            
                            # æ¸…ç†æ ‡é¢˜
                            title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                            title = re.sub(r'<[^>]+>', '', title).strip()
                            
                            # æå–é“¾æ¥
                            link_elem = item.find('link') or item.find('{http://www.w3.org/2005/Atom}link')
                            if link_elem is not None:
                                if link_elem.text:
                                    link = link_elem.text
                                elif 'href' in link_elem.attrib:
                                    link = link_elem.attrib['href']
                                else:
                                    link = ''
                            else:
                                link = ''
                            
                            # æå–å‘å¸ƒæ—¶é—´
                            pub_date_elem = item.find('pubDate') or item.find('published') or item.find('{http://www.w3.org/2005/Atom}published')
                            pub_date = pub_date_elem.text if pub_date_elem is not None else datetime.now().isoformat()
                            
                            # æå–æ‘˜è¦
                            description_elem = item.find('description') or item.find('summary') or item.find('{http://www.w3.org/2005/Atom}summary')
                            description = description_elem.text if description_elem is not None else ''
                            description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                            description = re.sub(r'<[^>]+>', '', description).strip()[:200]
                            
                            articles.append({
                                'title': title,
                                'url': link,
                                'source': source['name'],
                                'published': pub_date,
                                'summary': description,
                                'category': source['category'],
                                'tags': source.get('tags', []),
                                'description': source.get('description', '')
                            })
                            
                        except Exception as e:
                            continue
                            
        except Exception as e:
            print(f"ä» {source['name']} è·å–æ–‡ç« å¤±è´¥: {e}")
        
        return articles
    
    def fetch_all_articles(self, limit_per_source: int = 3) -> List[Dict[str, Any]]:
        """
        ä»æ‰€æœ‰æ•°æ®æºè·å–æ–‡ç« 
        
        Args:
            limit_per_source: æ¯ä¸ªæ•°æ®æºæœ€å¤šè·å–çš„æ–‡ç« æ•°
            
        Returns:
            æ‰€æœ‰æ–‡ç« åˆ—è¡¨
        """
        all_articles = []
        sources = self.get_all_sources()
        
        print(f"ğŸ” ä» {len(sources)} ä¸ª situation-monitor æ•°æ®æºè·å–æ–‡ç« ...")
        
        for i, source in enumerate(sources):
            try:
                articles = self.fetch_articles_from_source(source, limit=limit_per_source)
                all_articles.extend(articles)
                print(f"  âœ… {source['name']}: è·å– {len(articles)} ç¯‡æ–‡ç« ")
                time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
            except Exception as e:
                print(f"  âŒ {source['name']}: å¤±è´¥ - {e}")
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_articles.sort(
            key=lambda x: x.get('published', ''),
            reverse=True
        )
        
        print(f"ğŸ“Š æ€»å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
        return all_articles
    
    def generate_monitoring_report(self, articles: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆç›‘æ§é¢†åŸŸæŠ¥å‘Š
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            
        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        if not articles:
            return "ğŸ“­ æœªè·å–åˆ°ç›‘æ§é¢†åŸŸç›¸å…³æ–‡ç« "
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = {}
        for article in articles:
            category = article.get('category', 'æœªçŸ¥')
            if category not in categories:
                categories[category] = []
            categories[category].append(article)
        
        report = "ğŸ“Š Situation-Monitor é£æ ¼ç›‘æ§æ–°é—»æŠ¥å‘Š\n"
        report += "=" * 60 + "\n\n"
        report += f"ğŸ“… æŠ¥å‘Šæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“° æ–‡ç« æ€»æ•°: {len(articles)}\n"
        report += f"ğŸ·ï¸  ç±»åˆ«æ•°é‡: {len(categories)}\n\n"
        
        # æŒ‰ç±»åˆ«è¾“å‡º
        for category, cat_articles in categories.items():
            report += f"## {category} ({len(cat_articles)}ç¯‡)\n"
            
            for i, article in enumerate(cat_articles[:3], 1):  # æ¯ä¸ªç±»åˆ«æœ€å¤š3ç¯‡
                title = article.get('title', 'æ— æ ‡é¢˜')[:80]
                source = article.get('source', 'æœªçŸ¥æ¥æº')
                tags = article.get('tags', [])
                
                report += f"{i}. **{title}**\n"
                report += f"   æ¥æº: {source}\n"
                if tags:
                    report += f"   æ ‡ç­¾: {', '.join(tags[:3])}\n"
                report += "\n"
            
            if len(cat_articles) > 3:
                report += f"   è¿˜æœ‰ {len(cat_articles) - 3} ç¯‡æ–‡ç« ...\n"
            
            report += "\n"
        
        # çƒ­é—¨æ ‡ç­¾åˆ†æ
        all_tags = []
        for article in articles:
            all_tags.extend(article.get('tags', []))
        
        from collections import Counter
        tag_counts = Counter(all_tags)
        top_tags = tag_counts.most_common(5)
        
        if top_tags:
            report += "ğŸ”¥ çƒ­é—¨è¯é¢˜æ ‡ç­¾:\n"
            for tag, count in top_tags:
                report += f"   #{tag}: {count}æ¬¡\n"
        
        report += "\n" + "=" * 60
        report += "\nğŸ’¡ ä¸“æ³¨äºæŠ€æœ¯ç›‘æ§ã€å®‰å…¨ã€DevOpsã€å¯è§‚æµ‹æ€§ç­‰ä¸“ä¸šé¢†åŸŸ"
        
        return report


def test_situation_monitor_sources():
    """æµ‹è¯• situation-monitor æ•°æ®æº"""
    print("ğŸ§ª æµ‹è¯• situation-monitor æ•°æ®æº")
    print("=" * 60)
    
    sm_sources = SituationMonitorNewsSources()
    
    # 1. æµ‹è¯•æ•°æ®æºåŠ è½½
    sources = sm_sources.get_all_sources()
    print(f"ğŸ“‹ åŠ è½½ {len(sources)} ä¸ªæ•°æ®æº:")
    
    categories = {}
    for source in sources:
        category = source['category']
        categories[category] = categories.get(category, 0) + 1
    
    for category, count in categories.items():
        print(f"  â€¢ {category}: {count}ä¸ªæº")
    
    # 2. æµ‹è¯•è·å–æ–‡ç« ï¼ˆåªæµ‹è¯•å‰3ä¸ªæºï¼‰
    print(f"\nğŸ” æµ‹è¯•è·å–æ–‡ç« ï¼ˆå‰3ä¸ªæºï¼‰...")
    test_sources = sources[:3]
    total_articles = 0
    
    for source in test_sources:
        articles = sm_sources.fetch_articles_from_source(source, limit=2)
        print(f"  âœ… {source['name']}: {len(articles)}ç¯‡")
        total_articles += len(articles)
        time.sleep(1)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: ä»{len(test_sources)}ä¸ªæºè·å–{total_articles}ç¯‡æ–‡ç« ")
    
    # 3. å¦‚æœè·å–åˆ°æ–‡ç« ï¼Œç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    if total_articles > 0:
        all_articles = []
        for source in test_sources:
            all_articles.extend(sm_sources.fetch_articles_from_source(source, limit=2))
        
        report = sm_sources.generate_monitoring_report(all_articles)
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šé¢„è§ˆ (å‰200å­—ç¬¦):")
        print(report[:200] + "...")
    
    print("\nâœ… situation-monitor æ•°æ®æºæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_situation_monitor_sources()