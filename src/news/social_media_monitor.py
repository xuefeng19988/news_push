#!/usr/bin/env python3
"""
ç¤¾äº¤åª’ä½“ç›‘æ§æ¨¡å—
ä½¿ç”¨APIç®¡ç†å™¨ä»ç¯å¢ƒå˜é‡è¯»å–APIå¯†é’¥
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
from ..utils.api_manager import get_api_manager
from ..utils.logger import Logger

class SocialMediaMonitor:
    """ç¤¾äº¤åª’ä½“ç›‘æ§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¤¾äº¤åª’ä½“ç›‘æ§å™¨"""
        self.logger = Logger("SocialMediaMonitor").get_logger()
        self.api_mgr = get_api_manager()
        self.session = None
        
        # æ£€æŸ¥APIçŠ¶æ€
        self.check_api_status()
    
    def check_api_status(self):
        """æ£€æŸ¥APIçŠ¶æ€"""
        status = self.api_mgr.check_all_apis()
        
        self.logger.info("ç¤¾äº¤åª’ä½“APIçŠ¶æ€:")
        for api_name, api_status in status.items():
            if api_name in ["twitter", "weibo", "reddit"]:
                status_emoji = "âœ…" if api_status["enabled"] else "âŒ"
                self.logger.info(f"  {status_emoji} {api_name}: {api_status['status']}")
    
    def fetch_twitter_trends(self, location_id: int = 1) -> List[Dict[str, Any]]:
        """
        è·å–Twitterè¶‹åŠ¿
        
        Args:
            location_id: ä½ç½®ID (1=å…¨çƒ)
            
        Returns:
            è¶‹åŠ¿åˆ—è¡¨
        """
        if not self.api_mgr.is_api_enabled("twitter"):
            self.logger.warning("Twitter APIæœªå¯ç”¨ï¼Œè·³è¿‡è·å–è¶‹åŠ¿")
            return []
        
        try:
            import requests
            
            # è·å–APIé…ç½®
            headers = self.api_mgr.get_api_headers("twitter")
            url = self.api_mgr.get_api_url("twitter", f"trends/place.json?id={location_id}")
            
            if not headers.get("Authorization"):
                self.logger.error("Twitter Bearer Tokenæœªé…ç½®")
                return []
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                trends = []
                
                # è§£æè¶‹åŠ¿æ•°æ®
                for trend_data in data.get("trends", []):
                    trend = {
                        "name": trend_data.get("name", ""),
                        "url": trend_data.get("url", ""),
                        "tweet_volume": trend_data.get("tweet_volume", 0),
                        "source": "Twitter",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    trends.append(trend)
                
                self.logger.info(f"è·å–åˆ° {len(trends)} ä¸ªTwitterè¶‹åŠ¿")
                return trends[:10]  # è¿”å›å‰10ä¸ªè¶‹åŠ¿
            else:
                self.logger.error(f"Twitter APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"è·å–Twitterè¶‹åŠ¿å¤±è´¥: {e}")
            return []
    
    def fetch_weibo_hot_searches(self) -> List[Dict[str, Any]]:
        """
        è·å–å¾®åšçƒ­æœ
        
        Returns:
            çƒ­æœåˆ—è¡¨
        """
        if not self.api_mgr.is_api_enabled("weibo"):
            self.logger.warning("å¾®åšAPIæœªå¯ç”¨ï¼Œè·³è¿‡è·å–çƒ­æœ")
            return []
        
        try:
            import requests
            
            # å¾®åšçƒ­æœAPI
            url = "https://weibo.com/ajax/side/hotSearch"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "application/json",
            }
            
            # å¦‚æœæœ‰APIå¯†é’¥ï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
            weibo_headers = self.api_mgr.get_api_headers("weibo")
            headers.update(weibo_headers)
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                hot_searches = []
                
                # è§£æçƒ­æœæ•°æ®
                for item in data.get("data", {}).get("realtime", [])[:20]:
                    hot_search = {
                        "rank": item.get("rank", 0),
                        "keyword": item.get("word", ""),
                        "url": f"https://s.weibo.com/weibo?q={item.get('word', '')}",
                        "hot_value": item.get("num", 0),
                        "source": "å¾®åš",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    hot_searches.append(hot_search)
                
                self.logger.info(f"è·å–åˆ° {len(hot_searches)} ä¸ªå¾®åšçƒ­æœ")
                return hot_searches[:15]  # è¿”å›å‰15ä¸ªçƒ­æœ
            else:
                self.logger.error(f"å¾®åšAPIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"è·å–å¾®åšçƒ­æœå¤±è´¥: {e}")
            return []
    
    def fetch_reddit_hot_posts(self, subreddit: str = "all", limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–Redditçƒ­é—¨å¸–å­
        
        Args:
            subreddit: å­ç‰ˆå—åç§°
            limit: å¸–å­æ•°é‡é™åˆ¶
            
        Returns:
            å¸–å­åˆ—è¡¨
        """
        if not self.api_mgr.is_api_enabled("reddit"):
            self.logger.warning("Reddit APIæœªå¯ç”¨ï¼Œè·³è¿‡è·å–çƒ­é—¨å¸–å­")
            return []
        
        try:
            import requests
            
            # Reddit API (éœ€è¦OAuth2è®¤è¯ï¼Œè¿™é‡Œä½¿ç”¨å…¬å¼€API)
            url = f"https://www.reddit.com/r/{subreddit}/hot.json"
            params = {
                "limit": limit,
                "raw_json": 1
            }
            
            headers = {
                "User-Agent": "NewsPushSystem/0.0.1"
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                posts = []
                
                # è§£æå¸–å­æ•°æ®
                for post_data in data.get("data", {}).get("children", [])[:limit]:
                    post = post_data.get("data", {})
                    
                    reddit_post = {
                        "title": post.get("title", ""),
                        "url": f"https://reddit.com{post.get('permalink', '')}",
                        "score": post.get("score", 0),
                        "num_comments": post.get("num_comments", 0),
                        "subreddit": post.get("subreddit", ""),
                        "author": post.get("author", ""),
                        "created_utc": post.get("created_utc", 0),
                        "source": "Reddit",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    posts.append(reddit_post)
                
                self.logger.info(f"ä» r/{subreddit} è·å–åˆ° {len(posts)} ä¸ªçƒ­é—¨å¸–å­")
                return posts
            else:
                self.logger.error(f"Reddit APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"è·å–Redditçƒ­é—¨å¸–å­å¤±è´¥: {e}")
            return []
    
    def generate_social_media_report(self) -> str:
        """
        ç”Ÿæˆç¤¾äº¤åª’ä½“æŠ¥å‘Š
        
        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report_parts = ["ğŸ“± ç¤¾äº¤åª’ä½“åŠ¨æ€", ""]
        
        # è·å–å¾®åšçƒ­æœ
        weibo_hot_searches = self.fetch_weibo_hot_searches()
        if weibo_hot_searches:
            report_parts.append("ğŸ”¥ å¾®åšçƒ­æœ:")
            for i, search in enumerate(weibo_hot_searches[:5], 1):
                report_parts.append(f"{i}. {search['keyword']} ({search['hot_value']} çƒ­åº¦)")
            report_parts.append("")
        
        # è·å–Twitterè¶‹åŠ¿
        twitter_trends = self.fetch_twitter_trends()
        if twitter_trends:
            report_parts.append("ğŸ¦ Twitterè¶‹åŠ¿:")
            for i, trend in enumerate(twitter_trends[:5], 1):
                tweet_count = f" ({trend['tweet_volume']} æ¨æ–‡)" if trend.get('tweet_volume') else ""
                report_parts.append(f"{i}. {trend['name']}{tweet_count}")
            report_parts.append("")
        
        # è·å–Redditçƒ­é—¨
        reddit_posts = self.fetch_reddit_hot_posts("all", 5)
        if reddit_posts:
            report_parts.append("ğŸ“ Redditçƒ­é—¨:")
            for i, post in enumerate(reddit_posts[:3], 1):
                report_parts.append(f"{i}. {post['title']}")
                report_parts.append(f"   ğŸ‘ {post['score']} | ğŸ’¬ {post['num_comments']} | r/{post['subreddit']}")
            report_parts.append("")
        
        if len(report_parts) <= 2:  # åªæœ‰æ ‡é¢˜å’Œç©ºè¡Œ
            report_parts.append("æš‚æ—¶æ— æ³•è·å–ç¤¾äº¤åª’ä½“æ•°æ®")
            report_parts.append("è¯·é…ç½®ç›¸åº”çš„APIå¯†é’¥ä»¥å¯ç”¨åŠŸèƒ½")
        
        return "\n".join(report_parts)
    
    def run(self) -> str:
        """
        è¿è¡Œç¤¾äº¤åª’ä½“ç›‘æ§
        
        Returns:
            ç¤¾äº¤åª’ä½“æŠ¥å‘Š
        """
        self.logger.info("å¼€å§‹ç¤¾äº¤åª’ä½“ç›‘æ§")
        
        start_time = time.time()
        report = self.generate_social_media_report()
        
        duration = time.time() - start_time
        self.logger.info(f"ç¤¾äº¤åª’ä½“ç›‘æ§å®Œæˆï¼Œè€—æ—¶: {duration:.1f}ç§’")
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“± ç¤¾äº¤åª’ä½“ç›‘æ§ç³»ç»Ÿ")
    print("=" * 60)
    
    monitor = SocialMediaMonitor()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.run()
    
    print("\n" + report)
    print("\n" + "=" * 60)
    print("âœ… ç¤¾äº¤åª’ä½“ç›‘æ§å®Œæˆ")
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(main())