#!/usr/bin/env python3
"""
ç¤¾äº¤åª’ä½“ç›‘æ§å™¨ - ç›‘æ§Twitter/Xå’ŒRedditçƒ­ç‚¹
"""

import os
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SocialMediaMonitor:
    """ç¤¾äº¤åª’ä½“ç›‘æ§å™¨"""
    
    def __init__(self, config_file: str = "/home/admin/clawd/social_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        
        # æ•°æ®å­˜å‚¨
        self.data_dir = "/home/admin/clawd/social_data"
        os.makedirs(self.data_dir, exist_ok=True)
        
        # ç›‘æ§å…³é”®è¯ (è´¢ç»ç§‘æŠ€ç›¸å…³)
        self.keywords = [
            # è‚¡ç¥¨ç›¸å…³
            "stock", "stocks", "investing", "investment", "trading",
            "market", "markets", "finance", "financial",
            # å…¬å¸
            "Alibaba", "é˜¿é‡Œå·´å·´", "BABA", "Tencent", "è…¾è®¯", "Xiaomi", "å°ç±³",
            "BYD", "æ¯”äºšè¿ª", "Tesla", "TSLA", "Apple", "AAPL", "Google", "GOOGL",
            # ç§‘æŠ€
            "AI", "artificial intelligence", "tech", "technology", "innovation",
            "startup", "startups", "VC", "venture capital",
            # åŠ å¯†è´§å¸
            "Bitcoin", "BTC", "Ethereum", "ETH", "crypto", "cryptocurrency",
            "blockchain", "Web3",
            # ç»æµ
            "economy", "economic", "recession", "inflation", "Fed", "central bank"
        ]
        
        # ç”¨æˆ·ä»£ç†
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            "enabled": True,
            "check_interval_minutes": 15,
            "working_hours": {"start": 8, "end": 22},
            "twitter": {
                "enabled": True,
                "api_key": None,  # éœ€è¦Twitter APIå¯†é’¥
                "track_keywords": True,
                "max_tweets": 20
            },
            "reddit": {
                "enabled": True,
                "client_id": None,  # éœ€è¦Reddit APIå‡­è¯
                "client_secret": None,
                "subreddits": ["stocks", "investing", "technology", "finance", "CryptoCurrency"],
                "max_posts": 20
            },
            "notification": {
                "min_importance": 3,
                "max_items": 5,
                "channels": ["whatsapp"]
            }
        }
        
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                self.save_config(default_config)
                return default_config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return default_config
    
    def save_config(self, config: Dict):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
    
    def should_check(self) -> bool:
        """æ˜¯å¦åº”è¯¥æ£€æŸ¥"""
        if not self.config.get("enabled", True):
            return False
        
        # æ£€æŸ¥å·¥ä½œæ—¶é—´
        current_hour = datetime.now().hour
        working_hours = self.config.get("working_hours", {"start": 8, "end": 22})
        
        if current_hour < working_hours["start"] or current_hour >= working_hours["end"]:
            logger.info(f"â­ï¸ éå·¥ä½œæ—¶é—´ï¼Œè·³è¿‡ç¤¾äº¤åª’ä½“æ£€æŸ¥")
            return False
        
        return True
    
    def fetch_twitter_trends(self) -> List[Dict]:
        """è·å–Twitterè¶‹åŠ¿ (ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å…¬å¼€API)"""
        trends = []
        
        if not self.config.get("twitter", {}).get("enabled", True):
            return trends
        
        logger.info("ğŸ¦ è·å–Twitterè¶‹åŠ¿...")
        
        try:
            # æ–¹æ³•1: ä½¿ç”¨å…¬å¼€çš„Twitterè¶‹åŠ¿API (æœ‰é™åˆ¶)
            # æ³¨æ„: æ­£å¼ä½¿ç”¨éœ€è¦Twitter APIå¯†é’¥
            
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼Œå®é™…éœ€è¦Twitter API
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            
            mock_trends = [
                {
                    "name": "#StockMarket",
                    "tweet_volume": 125000,
                    "url": "https://twitter.com/search?q=%23StockMarket",
                    "importance": 4
                },
                {
                    "name": "#AI",
                    "tweet_volume": 89000,
                    "url": "https://twitter.com/search?q=%23AI",
                    "importance": 5
                },
                {
                    "name": "#Bitcoin",
                    "tweet_volume": 75000,
                    "url": "https://twitter.com/search?q=%23Bitcoin",
                    "importance": 4
                },
                {
                    "name": "#TechNews",
                    "tweet_volume": 52000,
                    "url": "https://twitter.com/search?q=%23TechNews",
                    "importance": 3
                }
            ]
            
            # è¿‡æ»¤ç›¸å…³è¶‹åŠ¿
            for trend in mock_trends:
                trend_name = trend["name"].lower().replace('#', '')
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                for keyword in self.keywords:
                    if keyword.lower() in trend_name:
                        trends.append(trend)
                        logger.info(f"  å‘ç°ç›¸å…³è¶‹åŠ¿: {trend['name']} ({trend['tweet_volume']}æ¨æ–‡)")
                        break
            
            logger.info(f"âœ… è·å– {len(trends)} ä¸ªç›¸å…³Twitterè¶‹åŠ¿")
            
        except Exception as e:
            logger.error(f"âŒ è·å–Twitterè¶‹åŠ¿å¤±è´¥: {e}")
        
        return trends
    
    def fetch_reddit_posts(self) -> List[Dict]:
        """è·å–Redditçƒ­é—¨å¸–å­"""
        posts = []
        
        if not self.config.get("reddit", {}).get("enabled", True):
            return posts
        
        logger.info("ğŸ“± è·å–Redditçƒ­é—¨å¸–å­...")
        
        try:
            # Reddit APIéœ€è¦è®¤è¯ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–æ–¹æ³•
            # å®é™…ä½¿ç”¨éœ€è¦Reddit APIå‡­è¯
            
            subreddits = self.config.get("reddit", {}).get("subreddits", ["stocks", "investing"])
            
            for subreddit in subreddits[:3]:  # é™åˆ¶æ£€æŸ¥çš„å­ç‰ˆå—æ•°é‡
                try:
                    # ä½¿ç”¨Redditçš„JSONç«¯ç‚¹ (å…¬å¼€ï¼Œæœ‰é™åˆ¶)
                    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                    
                    response = requests.get(url, headers=self.headers, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        for post in data.get("data", {}).get("children", [])[:5]:
                            post_data = post.get("data", {})
                            
                            title = post_data.get("title", "")
                            score = post_data.get("score", 0)
                            num_comments = post_data.get("num_comments", 0)
                            url = post_data.get("url", "")
                            permalink = f"https://reddit.com{post_data.get('permalink', '')}"
                            
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                            title_lower = title.lower()
                            relevant = False
                            
                            for keyword in self.keywords:
                                if keyword.lower() in title_lower:
                                    relevant = True
                                    break
                            
                            if relevant and score > 50:  # åªå…³æ³¨çƒ­é—¨å¸–å­
                                post_info = {
                                    "title": title,
                                    "subreddit": subreddit,
                                    "score": score,
                                    "comments": num_comments,
                                    "url": url if url.startswith('http') else permalink,
                                    "importance": self.calculate_importance(score, num_comments),
                                    "source": "reddit"
                                }
                                
                                posts.append(post_info)
                                logger.info(f"  å‘ç°çƒ­é—¨å¸–å­: r/{subreddit} - {title[:50]}... ({score}â†‘)")
                    
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    logger.error(f"âŒ è·å–r/{subreddit}å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… è·å– {len(posts)} ä¸ªç›¸å…³Redditå¸–å­")
            
        except Exception as e:
            logger.error(f"âŒ è·å–Redditå¸–å­å¤±è´¥: {e}")
        
        return posts
    
    def calculate_importance(self, score: int, comments: int) -> int:
        """è®¡ç®—å†…å®¹é‡è¦æ€§ (1-5åˆ†)"""
        importance = 1
        
        # åŸºäºåˆ†æ•°
        if score > 1000:
            importance += 2
        elif score > 100:
            importance += 1
        
        # åŸºäºè¯„è®ºæ•°
        if comments > 500:
            importance += 2
        elif comments > 50:
            importance += 1
        
        return min(max(importance, 1), 5)
    
    def analyze_sentiment(self, content: str) -> str:
        """ç®€å•æƒ…æ„Ÿåˆ†æ"""
        positive_words = ["bullish", "growth", "profit", "gain", "up", "positive", "good", "great", "buy"]
        negative_words = ["bearish", "loss", "drop", "down", "negative", "bad", "sell", "crash", "warning"]
        
        content_lower = content.lower()
        
        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def format_social_summary(self, trends: List[Dict], posts: List[Dict]) -> str:
        """æ ¼å¼åŒ–ç¤¾äº¤åª’ä½“æ‘˜è¦"""
        if not trends and not posts:
            return "ğŸ“­ å½“å‰æ— é‡è¦ç¤¾äº¤åª’ä½“åŠ¨æ€"
        
        summary = "ğŸŒ **ç¤¾äº¤åª’ä½“çƒ­ç‚¹æ‘˜è¦**\n\n"
        
        # Twitterè¶‹åŠ¿
        if trends:
            summary += "ğŸ¦ **Twitterè¶‹åŠ¿**:\n"
            
            for i, trend in enumerate(trends[:3], 1):
                name = trend["name"]
                volume = trend.get("tweet_volume", 0)
                importance = trend.get("importance", 1)
                
                summary += f"{i}. {name}\n"
                if volume:
                    summary += f"   ğŸ“Š {volume:,} æ¨æ–‡\n"
                summary += f"   â­ {'â˜…' * importance}\n\n"
        
        # Redditå¸–å­
        if posts:
            summary += "ğŸ“± **Redditçƒ­é—¨**:\n"
            
            for i, post in enumerate(posts[:3], 1):
                title = post["title"]
                subreddit = post["subreddit"]
                score = post["score"]
                comments = post["comments"]
                importance = post["importance"]
                
                # ç®€å•æƒ…æ„Ÿåˆ†æ
                sentiment = self.analyze_sentiment(title)
                sentiment_emoji = "ğŸ“ˆ" if sentiment == "positive" else "ğŸ“‰" if sentiment == "negative" else "ğŸ“Š"
                
                summary += f"{i}. {title[:60]}...\n"
                summary += f"   ğŸ“ r/{subreddit}\n"
                summary += f"   ğŸ‘ {score} â†‘ | ğŸ’¬ {comments}\n"
                summary += f"   {sentiment_emoji} {sentiment}\n"
                summary += f"   â­ {'â˜…' * importance}\n\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        summary += "---\n"
        summary += f"ğŸ“Š ç»Ÿè®¡: {len(trends)}è¶‹åŠ¿ + {len(posts)}å¸–å­\n"
        summary += f"â° æ›´æ–°æ—¶é—´: {datetime.now().strftime('%H:%M')}\n"
        summary += "ğŸ” ç›‘æ§å…³é”®è¯: è‚¡ç¥¨/æŠ•èµ„/ç§‘æŠ€/AI/åŠ å¯†è´§å¸\n"
        
        return summary
    
    def check_and_notify(self) -> Optional[str]:
        """æ£€æŸ¥å¹¶ç”Ÿæˆé€šçŸ¥"""
        if not self.should_check():
            return None
        
        logger.info("ğŸ” å¼€å§‹ç¤¾äº¤åª’ä½“ç›‘æ§...")
        
        try:
            # è·å–æ•°æ®
            twitter_trends = self.fetch_twitter_trends()
            reddit_posts = self.fetch_reddit_posts()
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self.format_social_summary(twitter_trends, reddit_posts)
            
            # ä¿å­˜æ•°æ®
            self.save_social_data(twitter_trends, reddit_posts)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é€šçŸ¥
            min_importance = self.config.get("notification", {}).get("min_importance", 3)
            
            has_important_content = False
            for trend in twitter_trends:
                if trend.get("importance", 1) >= min_importance:
                    has_important_content = True
                    break
            
            for post in reddit_posts:
                if post.get("importance", 1) >= min_importance:
                    has_important_content = True
                    break
            
            if has_important_content:
                logger.info("âœ… å‘ç°é‡è¦ç¤¾äº¤åª’ä½“åŠ¨æ€")
                
                # ä¿å­˜æ‘˜è¦æ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M")
                summary_file = f"./logs/social_summary_{timestamp}.txt"
                
                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write(summary)
                
                logger.info(f"ğŸ“ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
                return summary_file
            else:
                logger.info("ğŸ“­ æ— é‡è¦ç¤¾äº¤åª’ä½“åŠ¨æ€")
                return None
            
        except Exception as e:
            logger.error(f"âŒ ç¤¾äº¤åª’ä½“ç›‘æ§å¤±è´¥: {e}")
            return None
    
    def save_social_data(self, trends: List[Dict], posts: List[Dict]):
        """ä¿å­˜ç¤¾äº¤åª’ä½“æ•°æ®"""
        try:
            data = {
                "timestamp": datetime.now().isoformat(),
                "twitter_trends": trends,
                "reddit_posts": posts,
                "total_items": len(trends) + len(posts)
            }
            
            timestamp = datetime.now().strftime("%Y%m%d_%H")
            data_file = f"{self.data_dir}/social_data_{timestamp}.json"
            
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")

def test_social_monitor():
    """æµ‹è¯•ç¤¾äº¤åª’ä½“ç›‘æ§å™¨"""
    print("ğŸŒ æµ‹è¯•ç¤¾äº¤åª’ä½“ç›‘æ§å™¨...")
    
    monitor = SocialMediaMonitor()
    
    # æµ‹è¯•æ•°æ®è·å–
    print("\n1. è·å–Twitterè¶‹åŠ¿:")
    trends = monitor.fetch_twitter_trends()
    for trend in trends[:2]:
        print(f"   - {trend['name']} ({trend.get('tweet_volume', 0)}æ¨æ–‡)")
    
    print("\n2. è·å–Redditå¸–å­:")
    posts = monitor.fetch_reddit_posts()
    for post in posts[:2]:
        print(f"   - r/{post['subreddit']}: {post['title'][:50]}...")
    
    print("\n3. ç”Ÿæˆæ‘˜è¦:")
    summary = monitor.format_social_summary(trends, posts)
    print(summary[:200] + "..." if len(summary) > 200 else summary)
    
    print("\n4. å®Œæ•´æ£€æŸ¥:")
    result = monitor.check_and_notify()
    if result:
        print(f"âœ… æ£€æŸ¥å®Œæˆï¼Œæ‘˜è¦æ–‡ä»¶: {result}")
    else:
        print("âœ… æ£€æŸ¥å®Œæˆï¼Œæ— é‡è¦åŠ¨æ€")
    
    return monitor

if __name__ == "__main__":
    monitor = test_social_monitor()
    print("\nğŸŒ ç¤¾äº¤åª’ä½“ç›‘æ§å™¨æµ‹è¯•å®Œæˆ")