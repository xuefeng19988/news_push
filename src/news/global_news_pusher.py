#!/usr/bin/env python3
"""
å…¨çƒæ–°é—»æ¨é€ç³»ç»Ÿ - æ”¯æŒå›½å†…å¤–æ–°é—»æº
æ”¯æŒ: å›½å†…æ–°é—» + Twitter/X + Reddit + RSSæ–°é—»æº
"""

import os
import json
import time
import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GlobalNewsPusher:
    def __init__(self):
        self.news_file = "./logs/stock_data/news_history.json"
        self.news_cache = {}
        self.load_news_cache()
        
        # æ–°é—»æºé…ç½® - å¢å¼ºç‰ˆï¼Œæ›´å¤šæ–°é—»æº
        self.news_sources = {
            # ========== å›½é™…åª’ä½“ ==========
            "bbc": {
                "name": "BBCä¸­æ–‡ç½‘",
                "url": "https://www.bbc.com/zhongwen/simp/index.xml",
                "type": "rss",
                "enabled": True,
                "category": "å›½é™…"
            },
            "reuters": {
                "name": "è·¯é€ç¤¾ä¸­æ–‡",
                "url": "https://cn.reuters.com/rssFeed/CNTopGenNews/",
                "type": "rss",
                "enabled": True,
                "category": "å›½é™…"
            },
            "ft": {
                "name": "é‡‘èæ—¶æŠ¥ä¸­æ–‡",
                "url": "https://www.ftchinese.com/rss/news",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "nytimes": {
                "name": "çº½çº¦æ—¶æŠ¥ä¸­æ–‡",
                "url": "https://cn.nytimes.com/rss/",
                "type": "rss",
                "enabled": True,
                "category": "å›½é™…"
            },
            "bloomberg": {
                "name": "å½­åšç¤¾",
                "url": "https://www.bloomberg.com/feeds/bbiz/sitemap_news.xml",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "wsj": {
                "name": "åå°”è¡—æ—¥æŠ¥ä¸­æ–‡",
                "url": "https://cn.wsj.com/rss/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "economist": {
                "name": "ç»æµå­¦äºº",
                "url": "https://www.economist.com/rss",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            
            # ========== å›½å†…è´¢ç» ==========
            "wallstreetcn": {
                "name": "åå°”è¡—è§é—»",
                "url": "https://wallstreetcn.com/rss",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "caixin": {
                "name": "è´¢æ–°ç½‘",
                "url": "https://www.caixin.com/rss/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "eastmoney": {
                "name": "ä¸œæ–¹è´¢å¯Œ",
                "url": "https://rss.cnfol.com/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "sina_finance": {
                "name": "æ–°æµªè´¢ç»",
                "url": "https://rss.sina.com.cn/finance/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "hexun": {
                "name": "å’Œè®¯ç½‘",
                "url": "https://rss.hexun.com/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            "yicai": {
                "name": "ç¬¬ä¸€è´¢ç»",
                "url": "https://www.yicai.com/rss/",
                "type": "rss",
                "enabled": True,
                "category": "è´¢ç»"
            },
            
            # ========== ç§‘æŠ€æ–°é—» ==========
            "techcrunch": {
                "name": "TechCrunch",
                "url": "https://techcrunch.com/feed/",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            "theverge": {
                "name": "The Verge",
                "url": "https://www.theverge.com/rss/index.xml",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            "wired": {
                "name": "WIRED",
                "url": "https://www.wired.com/feed/rss",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            "arstechnica": {
                "name": "Ars Technica",
                "url": "https://arstechnica.com/feed/",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            "engadget": {
                "name": "Engadget",
                "url": "https://www.engadget.com/rss.xml",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            "techmeme": {
                "name": "Techmeme",
                "url": "https://www.techmeme.com/feed.xml",
                "type": "rss",
                "enabled": True,
                "category": "ç§‘æŠ€"
            },
            
            # ========== åŠ å¯†è´§å¸ ==========
            "coindesk": {
                "name": "CoinDesk",
                "url": "https://www.coindesk.com/arc/outboundfeeds/rss/",
                "type": "rss",
                "enabled": True,
                "category": "åŠ å¯†è´§å¸"
            },
            "cointelegraph": {
                "name": "CoinTelegraph",
                "url": "https://cointelegraph.com/rss",
                "type": "rss",
                "enabled": True,
                "category": "åŠ å¯†è´§å¸"
            },
            
            # ========== åˆ›ä¸šæŠ•èµ„ ==========
            "venturebeat": {
                "name": "VentureBeat",
                "url": "https://venturebeat.com/feed/",
                "type": "rss",
                "enabled": True,
                "category": "åˆ›ä¸š"
            },
            "techinasia": {
                "name": "Tech in Asia",
                "url": "https://www.techinasia.com/feed",
                "type": "rss",
                "enabled": True,
                "category": "åˆ›ä¸š"
            }
        }
        
        # å…³é”®è¯è¿‡æ»¤ (å¢å¼ºç‰ˆ)
        self.keywords = [
            # ========== è‚¡ç¥¨æŠ•èµ„ ==========
            "stock", "stocks", "share", "shares", "equity", "equities",
            "æŠ•èµ„", "invest", "investment", "portfolio", "trading", "trade",
            "è‚¡å¸‚", "stock market", "market", "markets", "exchange",
            "è¯åˆ¸", "securities", "broker", "brokerage", "trader",
            
            # ========== åŸºé‡‘ç†è´¢ ==========
            "åŸºé‡‘", "fund", "funds", "ETF", "mutual fund", "hedge fund",
            "ç†è´¢", "wealth", "wealth management", "asset", "assets",
            "å…»è€é‡‘", "pension", "retirement", "401k", "IRA",
            
            # ========== ç»æµé‡‘è ==========
            "ç»æµ", "economy", "economic", "economics", "GDP", "growth",
            "é‡‘è", "finance", "financial", "bank", "banking", "banker",
            "å¤®è¡Œ", "central bank", "Fed", "Federal Reserve", "ECB",
            "åˆ©ç‡", "interest rate", "rate", "rates", "yield", "bond",
            "é€šèƒ€", "inflation", "deflation", "CPI", "PPI", "price",
            "è´§å¸æ”¿ç­–", "monetary policy", "fiscal policy", "policy",
            " recession", "downturn", "slowdown", "crisis",
            
            # ========== ç§‘æŠ€ ==========
            "ç§‘æŠ€", "technology", "tech", "innovation", "innovative",
            "äººå·¥æ™ºèƒ½", "AI", "artificial intelligence", "machine learning",
            "äº’è”ç½‘", "internet", "web", "online", "digital", "digitization",
            "äº‘è®¡ç®—", "cloud", "cloud computing", "AWS", "Azure", "Google Cloud",
            "å¤§æ•°æ®", "big data", "data", "analytics", "analysis",
            "5G", "6G", "network", "telecom", "telecommunications",
            "èŠ¯ç‰‡", "chip", "semiconductor", "processor", "CPU", "GPU",
            "è½¯ä»¶", "software", "app", "application", "platform",
            "ç¡¬ä»¶", "hardware", "device", "smartphone", "computer",
            
            # ========== å…¬å¸ ==========
            "é˜¿é‡Œå·´å·´", "Alibaba", "BABA", "Tencent", "è…¾è®¯", "0700",
            "ç™¾åº¦", "Baidu", "BIDU", "å°ç±³", "Xiaomi", "1810",
            "åä¸º", "Huawei", "æ¯”äºšè¿ª", "BYD", "002594",
            "è‹¹æœ", "Apple", "AAPL", "è°·æ­Œ", "Google", "GOOGL", "GOOG",
            "å¾®è½¯", "Microsoft", "MSFT", "äºšé©¬é€Š", "Amazon", "AMZN",
            "ç‰¹æ–¯æ‹‰", "Tesla", "TSLA", "è‹±ä¼Ÿè¾¾", "NVIDIA", "NVDA",
            "Meta", "Facebook", "FB", "Netflix", "NFLX",
            
            # ========== å¸‚åœºæŒ‡æ•° ==========
            "ç¾è‚¡", "US stocks", "S&P", "S&P 500", "Dow", "Dow Jones",
            "çº³æ–¯è¾¾å…‹", "NASDAQ", "æ¸¯è‚¡", "Hong Kong stocks", "æ’ç”Ÿ", "Hang Seng",
            "Aè‚¡", "China stocks", "ä¸Šè¯", "Shanghai", "æ·±è¯", "Shenzhen",
            "åˆ›ä¸šæ¿", "ChiNext", "ç§‘åˆ›æ¿", "STAR Market",
            
            # ========== åŠ å¯†è´§å¸ ==========
            "æ¯”ç‰¹å¸", "Bitcoin", "BTC", "ä»¥å¤ªåŠ", "Ethereum", "ETH",
            "åŠ å¯†è´§å¸", "crypto", "cryptocurrency", "digital currency",
            "åŒºå—é“¾", "blockchain", "Web3", "DeFi", "NFT", "token",
            
            # ========== é‡è¦äº‹ä»¶ ==========
            "è´¢æŠ¥", "earnings", "quarterly", "Q1", "Q2", "Q3", "Q4",
            "ä¸šç»©", "performance", "results", "revenue", "profit", "loss",
            "æ”¶è´­", "acquisition", "acquire", "merger", "merge", "M&A",
            "åˆå¹¶", "consolidation", "partnership", "alliance",
            "ä¸Šå¸‚", "IPO", "initial public offering", "listing",
            "èèµ„", "funding", "raise", "capital", "venture", "VC",
            "è£å‘˜", "layoff", "layoffs", "firing", "fired",
            "æ¶¨ä»·", "price increase", "æ¶¨ä»·", "price hike",
            
            # ========== è¡Œä¸š ==========
            "æ±½è½¦", "auto", "automotive", "car", "EV", "electric vehicle",
            "æˆ¿åœ°äº§", "real estate", "property", "housing", "mortgage",
            "èƒ½æº", "energy", "oil", "gas", "petroleum", "renewable",
            "åŒ»ç–—", "healthcare", "medical", "pharma", "pharmaceutical",
            "æ¶ˆè´¹", "consumer", "retail", "e-commerce", "commerce",
            
            # ========== æ”¿ç­–ç›‘ç®¡ ==========
            "ç›‘ç®¡", "regulation", "regulatory", "supervision",
            "æ”¿ç­–", "policy", "law", "legislation", "bill",
            "ç¨æ”¶", "tax", "taxation", "tariff", "duty",
            "åˆ¶è£", "sanction", "sanctions", "embargo",
            
            # ========== åœ°ç¼˜æ”¿æ²» ==========
            "ä¸­ç¾", "US-China", "China-US", "trade war",
            "ä¿„ä¹Œ", "Russia-Ukraine", "Ukraine-Russia",
            "ä¸­ä¸œ", "Middle East", "Israel", "Palestine",
            "æ¬§ç›Ÿ", "EU", "European Union", "Brexit"
        ]
        
        # ç”¨æˆ·é…ç½®
        self.user_preferences = {
            "min_importance": 3,  # é‡è¦æ€§é˜ˆå€¼ (1-5)
            "max_articles": 5,    # æ¯æ¬¡æ¨é€æœ€å¤§æ–‡ç« æ•°
            "language": "zh",     # è¯­è¨€åå¥½
            "categories": ["è´¢ç»", "ç§‘æŠ€", "è‚¡ç¥¨"]  # å…³æ³¨çš„åˆ†ç±»
        }
    
    def load_news_cache(self):
        """åŠ è½½æ–°é—»å†å²è®°å½•"""
        try:
            if os.path.exists(self.news_file):
                with open(self.news_file, 'r', encoding='utf-8') as f:
                    self.news_cache = json.load(f)
            else:
                self.news_cache = {"articles": [], "last_update": None}
        except Exception as e:
            logger.error(f"åŠ è½½æ–°é—»ç¼“å­˜å¤±è´¥: {e}")
            self.news_cache = {"articles": [], "last_update": None}
    
    def save_news_cache(self):
        """ä¿å­˜æ–°é—»å†å²è®°å½•"""
        try:
            with open(self.news_file, 'w', encoding='utf-8') as f:
                json.dump(self.news_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°é—»ç¼“å­˜å¤±è´¥: {e}")
    
    def fetch_rss_news(self, source_name: str, source_config: dict) -> List[Dict]:
        """è·å–RSSæ–°é—»"""
        articles = []
        try:
            logger.info(f"ä» {source_name} è·å–RSSæ–°é—»...")
            feed = feedparser.parse(source_config["url"])
            
            for entry in feed.entries[:10]:  # å–æœ€æ–°10æ¡
                # è§£ææ–‡ç« ä¿¡æ¯
                article = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "summary": entry.get("summary", ""),
                    "published": entry.get("published", ""),
                    "source": source_name,
                    "source_name": source_config["name"],
                    "importance": self.calculate_importance(entry.get("title", "") + " " + entry.get("summary", "")),
                    "category": self.detect_category(entry.get("title", "")),
                    "timestamp": datetime.now().isoformat()
                }
                
                # è¿‡æ»¤é‡è¦æ–‡ç« 
                if article["importance"] >= self.user_preferences["min_importance"]:
                    articles.append(article)
                    logger.info(f"  å‘ç°é‡è¦æ–‡ç« : {article['title'][:50]}... (é‡è¦æ€§: {article['importance']})")
            
            logger.info(f"âœ… ä» {source_name} è·å– {len(articles)} æ¡é‡è¦æ–°é—»")
            
        except Exception as e:
            logger.error(f"âŒ è·å–RSSæ–°é—»å¤±è´¥ ({source_name}): {e}")
        
        return articles
    
    def fetch_web_news(self, source_name: str, source_config: dict) -> List[Dict]:
        """è·å–ç½‘é¡µæ–°é—» (å¤‡ç”¨æ–¹æ³•)"""
        articles = []
        try:
            logger.info(f"ä» {source_name} è·å–ç½‘é¡µæ–°é—»...")
            
            # ä½¿ç”¨requestsè·å–ç½‘é¡µ
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(source_config["url"], headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç®€å•è§£ææ ‡é¢˜ (è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™è°ƒæ•´)
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹
            title_match = re.search(r'<title>(.*?)</title>', response.text, re.IGNORECASE)
            if title_match:
                article = {
                    "title": title_match.group(1),
                    "link": source_config["url"],
                    "summary": "ä»ç½‘é¡µè·å–çš„æœ€æ–°æ–°é—»",
                    "published": datetime.now().isoformat(),
                    "source": source_name,
                    "source_name": source_config["name"],
                    "importance": 3,  # é»˜è®¤é‡è¦æ€§
                    "category": "ç»¼åˆ",
                    "timestamp": datetime.now().isoformat()
                }
                articles.append(article)
                logger.info(f"  è·å–æ–‡ç« : {article['title'][:50]}...")
            
            logger.info(f"âœ… ä» {source_name} è·å– {len(articles)} æ¡æ–°é—»")
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç½‘é¡µæ–°é—»å¤±è´¥ ({source_name}): {e}")
        
        return articles
    
    def calculate_importance(self, text: str) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§ (1-5åˆ†)"""
        importance = 1  # åŸºç¡€åˆ†
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        for keyword in self.keywords:
            if keyword.lower() in text.lower():
                importance += 1
        
        # æ ‡é¢˜é•¿åº¦å’Œå†…å®¹è´¨é‡
        if len(text) > 100:
            importance += 1
        
        # é™åˆ¶åœ¨1-5åˆ†
        return min(max(importance, 1), 5)
    
    def detect_category(self, title: str) -> str:
        """æ£€æµ‹æ–‡ç« åˆ†ç±»"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ["è‚¡ç¥¨", "è‚¡å¸‚", "æŠ•èµ„", "åŸºé‡‘", "ç†è´¢", "é‡‘è"]):
            return "è´¢ç»"
        elif any(word in title_lower for word in ["ç§‘æŠ€", "äººå·¥æ™ºèƒ½", "AI", "äº’è”ç½‘", "æ‰‹æœº", "ç”µè„‘"]):
            return "ç§‘æŠ€"
        elif any(word in title_lower for word in ["æ”¿æ²»", "æ”¿ç­–", "æ”¿åºœ", "å¤–äº¤"]):
            return "æ”¿æ²»"
        elif any(word in title_lower for word in ["ä½“è‚²", "è¶³çƒ", "ç¯®çƒ", "æ¯”èµ›"]):
            return "ä½“è‚²"
        elif any(word in title_lower for word in ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ˜æ˜Ÿ"]):
            return "å¨±ä¹"
        else:
            return "ç»¼åˆ"
    
    def is_new_article(self, article: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ–‡ç« """
        for cached_article in self.news_cache.get("articles", []):
            if (article["title"] == cached_article["title"] and 
                article["source"] == cached_article["source"]):
                return False
        return True
    
    def fetch_all_news(self) -> List[Dict]:
        """ä»æ‰€æœ‰æ–°é—»æºè·å–æ–°é—»"""
        all_articles = []
        
        logger.info("ğŸ“¡ å¼€å§‹è·å–å…¨çƒæ–°é—»...")
        
        for source_name, source_config in self.news_sources.items():
            if not source_config.get("enabled", True):
                continue
            
            try:
                if source_config["type"] == "rss":
                    articles = self.fetch_rss_news(source_name, source_config)
                elif source_config["type"] == "web":
                    articles = self.fetch_web_news(source_name, source_config)
                else:
                    continue
                
                # è¿‡æ»¤æ–°æ–‡ç« 
                new_articles = [article for article in articles if self.is_new_article(article)]
                all_articles.extend(new_articles)
                
                # æ›´æ–°ç¼“å­˜
                self.news_cache["articles"].extend(new_articles)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–°é—»æº {source_name} å¤±è´¥: {e}")
                continue
        
        # æŒ‰é‡è¦æ€§æ’åº
        all_articles.sort(key=lambda x: x["importance"], reverse=True)
        
        # é™åˆ¶æ–‡ç« æ•°é‡
        max_articles = self.user_preferences["max_articles"]
        if len(all_articles) > max_articles:
            all_articles = all_articles[:max_articles]
        
        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        self.news_cache["last_update"] = datetime.now().isoformat()
        
        # æ¸…ç†æ—§æ–‡ç«  (ä¿ç•™æœ€è¿‘100æ¡)
        if len(self.news_cache["articles"]) > 100:
            self.news_cache["articles"] = self.news_cache["articles"][-100:]
        
        # ä¿å­˜ç¼“å­˜
        self.save_news_cache()
        
        logger.info(f"âœ… å…±è·å– {len(all_articles)} æ¡æ–°æ–‡ç« ")
        return all_articles
    
    def format_news_message(self, articles: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ–°é—»æ¶ˆæ¯ (å¢å¼ºç‰ˆå†…å®¹)"""
        if not articles:
            return "ğŸ“­ æ²¡æœ‰æ–°çš„é‡è¦æ–°é—»éœ€è¦æ¨é€"
        
        message = "ğŸ“° **å…¨çƒé‡è¦æ–°é—»æ‘˜è¦**\n\n"
        
        for i, article in enumerate(articles, 1):
            # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            title = article['title']
            title = title.replace('"', "'").replace('`', "'").replace('\\', '')
            
            # æ ¼å¼åŒ–æ—¶é—´
            try:
                pub_time = datetime.fromisoformat(article["published"].replace('Z', '+00:00'))
                time_str = pub_time.strftime("%H:%M")
            except:
                time_str = "åˆšåˆš"
            
            # æ·»åŠ æ–‡ç« æ ‡é¢˜
            message += f"{i}. **{title}**\n"
            
            # æ¥æºå’Œæ—¶é—´
            message += f"   ğŸ“ æ¥æº: {article['source_name']}\n"
            message += f"   â° æ—¶é—´: {time_str}\n"
            
            # åˆ†ç±»
            category = article.get('category', 'ç»¼åˆ')
            message += f"   ğŸ·ï¸ åˆ†ç±»: {category}\n"
            
            # é‡è¦æ€§è¯„çº§
            importance = article['importance']
            importance_stars = 'â˜…' * importance
            message += f"   â­ é‡è¦æ€§: {importance_stars} ({importance}/5)\n"
            
            # è¯¦ç»†æ‘˜è¦ (å¢åŠ é•¿åº¦)
            if article.get("summary"):
                summary = article["summary"]
                # æ¸…ç†æ‘˜è¦
                summary = summary.replace('"', "'").replace('`', "'").replace('\\', '')
                
                # æ ¹æ®é‡è¦æ€§å†³å®šæ‘˜è¦é•¿åº¦
                if importance >= 4:
                    # é‡è¦æ–°é—»æ˜¾ç¤ºæ›´é•¿æ‘˜è¦
                    if len(summary) > 150:
                        summary = summary[:150] + "..."
                else:
                    if len(summary) > 100:
                        summary = summary[:100] + "..."
                
                message += f"   ğŸ“ æ‘˜è¦: {summary}\n"
            
            # å…³é”®ä¿¡æ¯æå– (å°è¯•ä»æ‘˜è¦ä¸­æå–)
            if article.get("summary"):
                summary_lower = article["summary"].lower()
                key_points = []
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
                finance_keywords = ["stock", "market", "invest", "price", "earnings", "revenue"]
                tech_keywords = ["ai", "artificial intelligence", "tech", "software", "hardware"]
                company_keywords = ["alibaba", "tencent", "xiaomi", "byd", "tesla", "apple"]
                
                for keyword in finance_keywords:
                    if keyword in summary_lower:
                        key_points.append("ğŸ’° è´¢ç»ç›¸å…³")
                        break
                
                for keyword in tech_keywords:
                    if keyword in summary_lower:
                        key_points.append("ğŸ¤– ç§‘æŠ€ç›¸å…³")
                        break
                
                for keyword in company_keywords:
                    if keyword in summary_lower:
                        key_points.append("ğŸ¢ å…¬å¸åŠ¨æ€")
                        break
                
                if key_points:
                    message += f"   ğŸ”‘ å…³é”®è¯: {', '.join(key_points[:2])}\n"
            
            # é“¾æ¥ - æ˜¾ç¤ºå®Œæ•´æ–‡ç« åœ°å€
            link = article['link']
            if link:
                # æ¸…ç†é“¾æ¥ï¼Œç¡®ä¿å¯ç‚¹å‡»
                link = link.strip()
                # WhatsAppä¸­é“¾æ¥å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                if len(link) < 150:  # é¿å…è¿‡é•¿é“¾æ¥
                    message += f"   ğŸ”— {link}\n"
                else:
                    # å¦‚æœé“¾æ¥å¤ªé•¿ï¼Œæ˜¾ç¤ºç¼©çŸ­ç‰ˆæœ¬ä½†ä¿ç•™å®Œæ•´é“¾æ¥
                    short_link = link[:80] + "..."
                    message += f"   ğŸ”— {short_link}\n"
                    # åœ¨æ¶ˆæ¯æœ«å°¾æ·»åŠ å®Œæ•´é“¾æ¥
                    # æ³¨æ„ï¼šè¿™éœ€è¦åœ¨æ¶ˆæ¯æ ¼å¼åŒ–å‡½æ•°ä¸­ç‰¹æ®Šå¤„ç†
            
            message += "\n"
        
        # å¢å¼ºç»Ÿè®¡ä¿¡æ¯
        message += "---\n"
        message += f"ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯**:\n"
        message += f"   â€¢ æ–‡ç« æ•°é‡: {len(articles)} æ¡é‡è¦æ–°é—»\n"
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for article in articles:
            cat = article.get('category', 'ç»¼åˆ')
            categories[cat] = categories.get(cat, 0) + 1
        
        if categories:
            message += f"   â€¢ åˆ†ç±»åˆ†å¸ƒ: "
            cat_list = [f"{cat}({count})" for cat, count in categories.items()]
            message += ", ".join(cat_list[:3]) + "\n"
        
        # é‡è¦æ€§ç»Ÿè®¡
        importance_counts = {}
        for article in articles:
            imp = article['importance']
            importance_counts[imp] = importance_counts.get(imp, 0) + 1
        
        if importance_counts:
            imp_list = [f"{'â˜…'*imp}({count})" for imp, count in sorted(importance_counts.items())]
            message += f"   â€¢ é‡è¦æ€§: {', '.join(imp_list)}\n"
        
        message += f"   â€¢ æ–°é—»æº: {len([s for s in self.news_sources.values() if s.get('enabled', True)])} ä¸ªæ´»è·ƒæº\n"
        message += f"   â€¢ æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
        message += f"   â€¢ æ¨é€é¢‘ç‡: æ¯å°æ—¶è‡ªåŠ¨æ¨é€\n"
        
        # æ·»åŠ ç®€è¦åˆ†æ
        if len(articles) >= 3:
            message += "\nğŸ“ˆ **ç®€è¦åˆ†æ**:\n"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜é‡è¦æ€§æ–°é—»
            high_importance = sum(1 for a in articles if a['importance'] >= 4)
            if high_importance > 0:
                message += f"   â€¢ æœ‰ {high_importance} æ¡é«˜é‡è¦æ€§æ–°é—»(4â˜…+)\n"
            
            # æ£€æŸ¥åˆ†ç±»åˆ†å¸ƒ
            if 'è´¢ç»' in categories:
                message += f"   â€¢ è´¢ç»æ–°é—»: {categories['è´¢ç»']} æ¡\n"
            if 'ç§‘æŠ€' in categories:
                message += f"   â€¢ ç§‘æŠ€æ–°é—»: {categories['ç§‘æŠ€']} æ¡\n"
        
        # æ£€æŸ¥æ¶ˆæ¯é•¿åº¦
        message_length = len(message)
        logger.info(f"ğŸ“± æ¶ˆæ¯é•¿åº¦: {message_length} å­—ç¬¦")
        
        if message_length > 4000:
            logger.warning("âš ï¸ æ¶ˆæ¯è¿‡é•¿ï¼Œè¿›è¡Œç²¾ç®€...")
            # ç²¾ç®€æ¶ˆæ¯ï¼Œä¿ç•™æ ¸å¿ƒå†…å®¹
            lines = message.split('\n')
            simplified = []
            for line in lines:
                if len(''.join(simplified)) + len(line) < 3500:
                    simplified.append(line)
                else:
                    break
            
            message = '\n'.join(simplified)
            message += "\n... (å†…å®¹å·²ç²¾ç®€)\n"
        
        return message
    
    def run(self):
        """è¿è¡Œæ–°é—»æ¨é€"""
        logger.info("ğŸš€ å…¨çƒæ–°é—»æ¨é€ç³»ç»Ÿå¯åŠ¨")
        
        try:
            # è·å–æ–°é—»
            articles = self.fetch_all_news()
            
            if not articles:
                logger.info("ğŸ“­ æ²¡æœ‰æ–°çš„é‡è¦æ–°é—»éœ€è¦æ¨é€")
                return None
            
            # æ ¼å¼åŒ–æ¶ˆæ¯
            message = self.format_news_message(articles)
            
            # ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M")
            message_file = f"./logs/news_message_{timestamp}.txt"
            
            with open(message_file, 'w', encoding='utf-8') as f:
                f.write(message)
            
            logger.info(f"âœ… æ–°é—»æ¶ˆæ¯å·²ä¿å­˜: {message_file}")
            
            # åŒæ—¶ä¿å­˜åˆ°å¾…å‘é€é˜Ÿåˆ—
            pending_file = f"./logs/pending_news_{timestamp}.txt"
            with open(pending_file, 'w', encoding='utf-8') as f:
                f.write(message)
            
            logger.info(f"âœ… æ–°é—»å·²æ·»åŠ åˆ°å¾…å‘é€é˜Ÿåˆ—: {pending_file}")
            
            return message_file
            
        except Exception as e:
            logger.error(f"âŒ æ–°é—»æ¨é€ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    pusher = GlobalNewsPusher()
    result = pusher.run()
    
    if result:
        print(f"âœ… æ–°é—»æ¨é€å®Œæˆ: {result}")
        # è¯»å–å¹¶æ˜¾ç¤ºæ¶ˆæ¯
        with open(result, 'r', encoding='utf-8') as f:
            print(f.read())
    else:
        print("âŒ æ–°é—»æ¨é€å¤±è´¥æˆ–æ²¡æœ‰æ–°æ–°é—»")

if __name__ == "__main__":
    main()