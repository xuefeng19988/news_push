#!/usr/bin/env python3
"""
æ–°é—»+è‚¡ç¥¨æ¨é€ç³»ç»Ÿ - é›†æˆç‰ˆæœ¬
æ¯å°æ—¶æ¨é€æ–°é—»å’Œè‚¡ç¥¨ä¿¡æ¯åˆ°WhatsApp
"""

import os
import sys
import json
import time
import requests
from datetime import datetime, timedelta
import sqlite3
import hashlib
import re

class NewsStockPusher:
    """æ–°é—»+è‚¡ç¥¨æ¨é€å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
        
        # æ•°æ®åº“è·¯å¾„
        self.db_path = "/home/admin/clawd/news_cache.db"
        self.init_database()
        
        # ç›‘æ§çš„è‚¡ç¥¨
        self.stocks = [
            {
                "name": "é˜¿é‡Œå·´å·´-W",
                "symbol": "09988.HK",
                "yahoo_symbol": "9988.HK",
                "currency": "HKD"
            },
            {
                "name": "å°ç±³é›†å›¢-W", 
                "symbol": "01810.HK",
                "yahoo_symbol": "1810.HK",
                "currency": "HKD"
            },
            {
                "name": "æ¯”äºšè¿ª",
                "symbol": "002594.SZ",
                "yahoo_symbol": "002594.SZ",
                "currency": "CNY"
            }
        ]
        
        # æ–°é—»æº - å›½å†…+å›½é™…+ç¤¾äº¤åª’ä½“
        self.news_sources = [
            # å›½å†…æ–°é—»åª’ä½“
            {
                'name': 'æ–°æµªæ–°é—»',
                'type': 'rss',
                'url': 'http://rss.sina.com.cn/news/marquee/ddt.xml',
                'category': 'å›½å†…åª’ä½“'
            },
            {
                'name': 'ç½‘æ˜“æ–°é—»',
                'type': 'rss', 
                'url': 'http://news.163.com/special/00011K6L/rss_newsattitude.xml',
                'category': 'å›½å†…åª’ä½“'
            },
            {
                'name': 'å‡¤å‡°æ–°é—»',
                'type': 'rss',
                'url': 'https://news.ifeng.com/rss/ifengnews.xml',
                'category': 'å›½å†…åª’ä½“'
            },
            {
                'name': 'æ¾æ¹ƒæ–°é—»',
                'type': 'rss',
                'url': 'https://www.thepaper.cn/rss_hot.jsp',
                'category': 'å›½å†…åª’ä½“'
            },
            {
                'name': 'ä»Šæ—¥å¤´æ¡çƒ­æ¦œ',
                'type': 'api',
                'url': 'https://www.toutiao.com/hot-event/hot-board/',
                'category': 'ç¤¾äº¤åª’ä½“'
            },
            
            # å›½é™…æ–°é—»åª’ä½“
            {
                'name': 'BBCä¸­æ–‡ç½‘',
                'type': 'rss',
                'url': 'https://www.bbc.com/zhongwen/simp/index.xml',
                'category': 'å›½é™…åª’ä½“'
            },
            {
                'name': 'BBC World',
                'type': 'rss',
                'url': 'http://feeds.bbci.co.uk/news/world/rss.xml',
                'category': 'å›½é™…åª’ä½“'
            },
            {
                'name': 'CNNå›½é™…ç‰ˆ',
                'type': 'rss',
                'url': 'http://rss.cnn.com/rss/edition_world.rss',
                'category': 'å›½é™…åª’ä½“'
            },
            {
                'name': 'é‡‘èæ—¶æŠ¥ä¸­æ–‡',
                'type': 'rss',
                'url': 'https://www.ftchinese.com/rss/news',
                'category': 'å›½é™…è´¢ç»'
            },
            {
                'name': 'æ—¥ç»äºšæ´²',
                'type': 'rss',
                'url': 'https://asia.nikkei.com/rss',
                'category': 'äºšæ´²åª’ä½“'
            },
            {
                'name': 'å—åæ—©æŠ¥',
                'type': 'rss',
                'url': 'https://www.scmp.com/rss/91/feed',
                'category': 'äºšæ´²åª’ä½“'
            },
            
            # ç¤¾äº¤åª’ä½“å¹³å°ï¼ˆéœ€è¦APIæˆ–ç‰¹æ®Šå¤„ç†ï¼‰
            {
                'name': 'å¾®åšçƒ­æœ',
                'type': 'api',
                'url': 'https://weibo.com/ajax/side/hotSearch',
                'category': 'ç¤¾äº¤åª’ä½“'
            },
            {
                'name': 'Twitterè¶‹åŠ¿',
                'type': 'api',
                'url': 'https://api.twitter.com/1.1/trends/place.json?id=1',
                'category': 'ç¤¾äº¤åª’ä½“'
            },
            {
                'name': 'Redditçƒ­é—¨',
                'type': 'api',
                'url': 'https://www.reddit.com/r/all/hot.json',
                'category': 'ç¤¾äº¤åª’ä½“'
            }
        ]
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºå·²æ¨é€æ–‡ç« è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pushed_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_hash TEXT UNIQUE,
                title TEXT,
                source TEXT,
                url TEXT,
                pushed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                category TEXT
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_article_hash ON pushed_articles(article_hash)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_pushed_at ON pushed_articles(pushed_at)
        ''')
        
        # è‡ªåŠ¨æ¸…ç†7å¤©å‰çš„æ—§è®°å½•
        seven_days_ago = datetime.now() - timedelta(days=7)
        cursor.execute(
            "DELETE FROM pushed_articles WHERE pushed_at < ?",
            (seven_days_ago.strftime('%Y-%m-%d %H:%M:%S'),)
        )
        
        deleted_count = cursor.rowcount
        if deleted_count > 0:
            print(f"ğŸ—‘ï¸  è‡ªåŠ¨æ¸…ç†äº† {deleted_count} æ¡7å¤©å‰çš„æ—§è®°å½•")
        
        conn.commit()
        conn.close()
    
    def get_article_hash(self, title: str, url: str) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€å“ˆå¸Œ"""
        content = f"{title}|{url}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def is_article_pushed(self, article_hash: str) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²æ¨é€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT 1 FROM pushed_articles WHERE article_hash = ?",
            (article_hash,)
        )
        
        result = cursor.fetchone()
        conn.close()
        
        return result is not None
    
    def mark_article_pushed(self, article: dict):
        """æ ‡è®°æ–‡ç« ä¸ºå·²æ¨é€"""
        article_hash = self.get_article_hash(article['title'], article['url'])
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO pushed_articles 
                (article_hash, title, source, url, pushed_at, category)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                article_hash,
                article['title'],
                article.get('source', 'æœªçŸ¥'),
                article['url'],
                datetime.now().isoformat(),
                article.get('category', 'ç»¼åˆ')
            ))
            
            conn.commit()
        except sqlite3.IntegrityError:
            # æ–‡ç« å·²å­˜åœ¨ï¼Œå¿½ç•¥
            pass
        finally:
            conn.close()
    
    def cleanup_old_records(self, days_to_keep: int = 7):
        """æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„æ—§è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        cursor.execute(
            "DELETE FROM pushed_articles WHERE pushed_at < ?",
            (cutoff_date.strftime('%Y-%m-%d %H:%M:%S'),)
        )
        
        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        if deleted_count > 0:
            print(f"ğŸ—‘ï¸  æ¸…ç†äº† {deleted_count} æ¡{days_to_keep}å¤©å‰çš„æ—§è®°å½•")
        
        return deleted_count
    
    def fetch_news_from_rss(self, rss_url: str, source: str) -> list:
        """ä»RSSæºè·å–æ–°é—»ï¼ˆæ”¯æŒå¤šç§RSSæ ¼å¼ï¼‰"""
        articles = []
        try:
            response = self.session.get(rss_url, timeout=15)
            if response.status_code == 200:
                content = response.text
                
                # å°è¯•å¤šç§RSSæ ¼å¼è§£æ
                import re
                
                # æ–¹æ³•1: æ ‡å‡†RSSæ ¼å¼ <item>...</item>
                items = re.findall(r'<item>(.*?)</item>', content, re.DOTALL)
                
                # æ–¹æ³•2: Atomæ ¼å¼ <entry>...</entry>
                if not items:
                    items = re.findall(r'<entry>(.*?)</entry>', content, re.DOTALL)
                
                # æ–¹æ³•3: å…¶ä»–å¸¸è§æ ¼å¼
                if not items:
                    # å°è¯•æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ¡ç›®
                    items = re.findall(r'<(?:item|entry)>(.*?)</(?:item|entry)>', content, re.DOTALL)
                
                for item in items[:8]:  # æ¯ä¸ªæºå–8æ¡
                    # æå–æ ‡é¢˜ï¼ˆå°è¯•å¤šç§æ ‡ç­¾ï¼‰
                    title = None
                    for tag in ['title', 'dc:title']:
                        match = re.search(f'<{tag}>(.*?)</{tag}>', item, re.DOTALL)
                        if match:
                            title = match.group(1).strip()
                            break
                    
                    if not title:
                        continue
                    
                    # æå–é“¾æ¥ï¼ˆå°è¯•å¤šç§æ ‡ç­¾å’Œå±æ€§ï¼‰
                    url = None
                    
                    # å°è¯• <link>æ ‡ç­¾
                    link_match = re.search(r'<link>(.*?)</link>', item)
                    if link_match:
                        url = link_match.group(1).strip()
                    else:
                        # å°è¯• <link href="...">
                        link_match = re.search(r'<link[^>]*href=["\']([^"\']+)["\'][^>]*>', item)
                        if link_match:
                            url = link_match.group(1).strip()
                        else:
                            # å°è¯• <guid>
                            guid_match = re.search(r'<guid[^>]*>(.*?)</guid>', item)
                            if guid_match and guid_match.group(1).startswith('http'):
                                url = guid_match.group(1).strip()
                    
                    if not url:
                        continue
                    
                    # æå–æè¿°/å†…å®¹ï¼ˆå°è¯•å¤šç§æ ‡ç­¾ï¼‰
                    description = ""
                    for tag in ['description', 'content:encoded', 'content', 'summary', 'dc:description']:
                        match = re.search(f'<{tag}>(.*?)</{tag}>', item, re.DOTALL)
                        if match:
                            description = match.group(1).strip()
                            break
                    
                    # æå–å‘å¸ƒæ—¶é—´
                    pub_date = ""
                    for tag in ['pubDate', 'dc:date', 'published', 'updated']:
                        match = re.search(f'<{tag}>(.*?)</{tag}>', item)
                        if match:
                            pub_date = match.group(1).strip()
                            break
                    
                    # æ¸…ç†æ ‡é¢˜ä¸­çš„CDATAå’ŒHTMLæ ‡ç­¾
                    title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                    title = re.sub(r'<[^>]+>', '', title)
                    
                    # æ¸…ç†æè¿°ä¸­çš„HTMLæ ‡ç­¾
                    if description:
                        description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                        description = re.sub(r'<[^>]+>', '', description)
                        description = re.sub(r'\s+', ' ', description).strip()
                    
                    articles.append({
                        'title': title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        'url': url,
                        'description': description[:300] if description else "",  # é™åˆ¶æè¿°é•¿åº¦
                        'pub_date': pub_date,
                        'source': source,
                        'category': 'ç»¼åˆ'  # å°†åœ¨å¤–éƒ¨è®¾ç½®
                    })
            
            return articles
            
        except Exception as e:
            print(f"âŒ è·å–RSSæ–°é—»å¤±è´¥ ({source}): {e}")
            return []
    
    def fetch_news_from_api(self, api_url: str, source: str) -> list:
        """ä»APIè·å–æ–°é—»å’Œç¤¾äº¤åª’ä½“å†…å®¹"""
        articles = []
        try:
            headers = self.session.headers.copy()
            
            # ä¸ºä¸åŒå¹³å°è®¾ç½®ç‰¹å®šçš„è¯·æ±‚å¤´
            if 'weibo' in api_url:
                headers.update({
                    'Referer': 'https://weibo.com/',
                    'Accept': 'application/json, text/plain, */*'
                })
            elif 'twitter' in api_url:
                # Twitter APIéœ€è¦è®¤è¯ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                print(f"  âš ï¸ Twitter APIéœ€è¦è®¤è¯ï¼Œè·³è¿‡")
                return []
            elif 'reddit' in api_url:
                headers.update({
                    'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'
                })
            
            response = self.session.get(api_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                
                # æ ¹æ®ä¸åŒçš„APIæ ¼å¼è§£æ
                if 'weibo' in api_url:
                    # å¾®åšçƒ­æœæ¦œæ ¼å¼
                    if 'data' in data and 'realtime' in data['data']:
                        for item in data['data']['realtime'][:10]:
                            word = item.get('word', '')
                            if word:
                                articles.append({
                                    'title': f"#{word}",
                                    'url': f"https://s.weibo.com/weibo?q={word}",
                                    'description': item.get('word_scheme', ''),
                                    'source': source,
                                    'category': 'ç¤¾äº¤åª’ä½“'
                                })
                
                elif 'reddit' in api_url:
                    # Redditçƒ­é—¨å¸–å­æ ¼å¼
                    if 'data' in data and 'children' in data['data']:
                        for child in data['data']['children'][:10]:
                            post = child.get('data', {})
                            title = post.get('title', '')
                            url = post.get('url', '')
                            
                            if title and url:
                                articles.append({
                                    'title': title[:150],
                                    'url': f"https://reddit.com{post.get('permalink', '')}",
                                    'description': f"ğŸ‘ {post.get('ups', 0)} | ğŸ’¬ {post.get('num_comments', 0)}",
                                    'source': source,
                                    'category': 'ç¤¾äº¤åª’ä½“'
                                })
                
                elif 'toutiao' in api_url or 'ä»Šæ—¥å¤´æ¡' in source:
                    # ä»Šæ—¥å¤´æ¡çƒ­æ¦œæ ¼å¼
                    if 'data' in data:
                        for item in data['data'][:10]:
                            title = item.get('Title', '')
                            url = item.get('Url', '')
                            
                            if title and url:
                                articles.append({
                                    'title': title.strip(),
                                    'url': url.strip(),
                                    'description': item.get('Description', ''),
                                    'source': source,
                                    'category': 'ç¤¾äº¤åª’ä½“'
                                })
                
                return articles
            else:
                print(f"  âš ï¸ APIè¿”å›é”™è¯¯: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ è·å–APIå†…å®¹å¤±è´¥ ({source}): {e}")
            return []
    
    def fetch_all_news(self) -> list:
        """è·å–æ‰€æœ‰æ–°é—»"""
        print("ğŸ“¡ å¼€å§‹è·å–æ–°é—»...")
        
        all_articles = []
        total_sources = len(self.news_sources)
        
        for i, source in enumerate(self.news_sources, 1):
            print(f"  [{i}/{total_sources}] ä» {source['name']} è·å–...")
            
            articles = []
            if source['type'] == 'rss':
                articles = self.fetch_news_from_rss(source['url'], source['name'])
            elif source['type'] == 'api':
                articles = self.fetch_news_from_api(source['url'], source['name'])
            
            # æ·»åŠ åˆ†ç±»ä¿¡æ¯
            for article in articles:
                article['category'] = source['category']
            
            all_articles.extend(articles)
            
            # é¿å…è¯·æ±‚è¿‡å¿«ï¼Œä½†ä¸åŒæºä¹‹é—´ç­‰å¾…æ—¶é—´ä¸åŒ
            if i < total_sources:
                time.sleep(0.3)  # å‡å°‘ç­‰å¾…æ—¶é—´
        
        print(f"âœ… å…±è·å– {len(all_articles)} æ¡æ–°é—»")
        
        # æŒ‰æ¥æºåˆ†ç±»ç»Ÿè®¡
        source_stats = {}
        for article in all_articles:
            source = article['source']
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print("ğŸ“Š æ–°é—»æ¥æºç»Ÿè®¡:")
        for source, count in source_stats.items():
            print(f"  {source}: {count}æ¡")
        
        return all_articles
    
    def filter_new_articles(self, articles: list) -> list:
        """è¿‡æ»¤å‡ºæ–°æ–‡ç« ï¼ˆæœªæ¨é€è¿‡çš„ï¼‰"""
        new_articles = []
        
        for article in articles:
            article_hash = self.get_article_hash(article['title'], article['url'])
            
            if not self.is_article_pushed(article_hash):
                new_articles.append(article)
        
        print(f"ğŸ“Š è¿‡æ»¤åæ–°æ–‡ç« : {len(new_articles)}/{len(articles)} æ¡")
        return new_articles
    
    def get_stock_from_yahoo(self, stock_info: dict):
        """ä»Yahoo Financeè·å–è‚¡ç¥¨æ•°æ®"""
        try:
            symbol = stock_info["yahoo_symbol"]
            url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
            
            params = {
                "interval": "1d",
                "range": "1d",
                "includePrePost": "false"
            }
            
            response = self.session.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                chart_data = data.get("chart", {}).get("result", [{}])[0]
                meta = chart_data.get("meta", {})
                quotes = chart_data.get("indicators", {}).get("quote", [{}])[0]
                
                if meta and quotes:
                    closes = quotes.get("close", [])
                    if closes:
                        latest_price = closes[-1]
                        prev_price = closes[-2] if len(closes) > 1 else latest_price
                        
                        change = latest_price - prev_price
                        change_percent = (change / prev_price) * 100 if prev_price else 0
                        
                        return {
                            "symbol": stock_info["symbol"],
                            "name": stock_info["name"],
                            "price": latest_price,
                            "change": change,
                            "change_percent": change_percent,
                            "currency": stock_info["currency"],
                            "timestamp": datetime.now().isoformat()
                        }
            
            return None
                
        except Exception as e:
            print(f"âŒ Yahoo APIé”™è¯¯ ({stock_info['symbol']}): {e}")
            return None
    
    def get_all_stocks_data(self):
        """è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®"""
        print("ğŸ“ˆ å¼€å§‹è·å–è‚¡ç¥¨æ•°æ®...")
        
        all_data = []
        
        for stock in self.stocks:
            print(f"  è·å– {stock['name']} ({stock['symbol']})...")
            data = self.get_stock_from_yahoo(stock)
            
            if data:
                all_data.append(data)
                print(f"    âœ… æˆåŠŸ: {data['price']} {data['currency']}")
            else:
                print(f"    âŒ å¤±è´¥")
        
        return all_data
    
    def analyze_stock_sentiment(self, change_percent: float) -> str:
        """åˆ†æè‚¡ç¥¨æƒ…ç»ª"""
        if change_percent > 3:
            return "ğŸš€ éå¸¸æ­£é¢"
        elif change_percent > 1:
            return "ğŸ“ˆ æ­£é¢"
        elif change_percent > -1:
            return "â¡ï¸ ä¸­æ€§"
        elif change_percent > -3:
            return "ğŸ“‰ è´Ÿé¢"
        else:
            return "ğŸ”» éå¸¸è´Ÿé¢"
    
    def generate_summary(self, description: str, max_length: int = 150) -> str:
        """ç”Ÿæˆè¯¦ç»†æ–‡ç« æ‘˜è¦"""
        if not description or description.strip() == '':
            return "æš‚æ— è¯¦ç»†å†…å®¹æ‘˜è¦"
        
        # æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
        clean_text = re.sub(r'<[^>]+>', '', description)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # ç§»é™¤å¸¸è§çš„æ— ç”¨å‰ç¼€
        prefixes = ['æ‘˜è¦ï¼š', 'ç®€ä»‹ï¼š', 'å†…å®¹ï¼š', 'å¯¼è¯»ï¼š', 'ã€', '[']
        for prefix in prefixes:
            if clean_text.startswith(prefix):
                clean_text = clean_text[len(prefix):].strip()
        
        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        if len(clean_text) <= 50:
            return clean_text
        
        # å°è¯•æå–å…³é”®å¥å­ï¼ˆç¬¬ä¸€å¥+æœ€åä¸€å¥ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', clean_text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) >= 2:
            # å–ç¬¬ä¸€å¥å’Œæœ€åä¸€å¥
            first_sentence = sentences[0]
            last_sentence = sentences[-1]
            
            # å¦‚æœç¬¬ä¸€å¥å’Œæœ€åä¸€å¥ç›¸åŒæˆ–ç›¸ä¼¼ï¼Œåªå–ç¬¬ä¸€å¥
            if first_sentence == last_sentence or last_sentence in first_sentence:
                summary = first_sentence
            else:
                summary = f"{first_sentence}...{last_sentence}"
        elif sentences:
            summary = sentences[0]
        else:
            summary = clean_text
        
        # æˆªå–æŒ‡å®šé•¿åº¦
        if len(summary) > max_length:
            summary = summary[:max_length] + "..."
        
        return summary
    
    def enhance_article_info(self, article: dict) -> dict:
        """å¢å¼ºæ–‡ç« ä¿¡æ¯ï¼ˆåŒ…å«æ›´æ–°æ—¶é—´ã€é‡è¦æ€§è¯„çº§ç­‰ï¼‰"""
        enhanced = article.copy()
        
        # æ ¹æ®æ¥æºæ·»åŠ é¢å¤–ä¿¡æ¯
        source = article.get('source', '')
        description = article.get('description', '')
        pub_date = article.get('pub_date', '')
        
        # 1. æå–å…³é”®ä¿¡æ¯æ ‡ç­¾
        extra_info_tags = []
        
        if 'å¾®åš' in source:
            extra_info_tags.append("ğŸ”¥ å®æ—¶çƒ­ç‚¹")
        elif 'Reddit' in source:
            extra_info_tags.append("ğŸ‘¥ ç¤¾åŒºçƒ­è®®")
        elif 'BBC' in source or 'CNN' in source:
            extra_info_tags.append("ğŸŒ å›½é™…æƒå¨")
        elif 'é‡‘èæ—¶æŠ¥' in source or 'åå°”è¡—' in source:
            extra_info_tags.append("ğŸ’¼ è´¢ç»æ·±åº¦")
        elif 'æ¾æ¹ƒ' in source:
            extra_info_tags.append("ğŸ“Š æ·±åº¦è°ƒæŸ¥")
        elif 'å¤´æ¡' in source:
            extra_info_tags.append("ğŸ“± å¹³å°çƒ­æ¦œ")
        
        # 2. åˆ†ææ–‡ç« é‡è¦æ€§
        importance_score = self.calculate_importance_score(article)
        importance_level = self.get_importance_level(importance_score)
        enhanced['importance'] = importance_level
        
        # 3. å¤„ç†æ›´æ–°æ—¶é—´
        update_time = self.parse_publication_time(pub_date)
        enhanced['update_time'] = update_time
        enhanced['time_recency'] = self.get_time_recency(update_time)
        
        # 4. ç»„åˆé¢å¤–ä¿¡æ¯
        if extra_info_tags:
            enhanced['extra_info'] = " | ".join(extra_info_tags)
        
        # 5. æ·»åŠ é˜…è¯»æ—¶é—´ä¼°è®¡
        title_len = len(article.get('title', ''))
        desc_len = len(description)
        total_chars = title_len + desc_len
        read_time = max(1, total_chars // 500)  # æŒ‰500å­—/åˆ†é’Ÿè®¡ç®—
        enhanced['read_time'] = f"â±ï¸ é˜…è¯»çº¦{read_time}åˆ†é’Ÿ"
        
        return enhanced
    
    def calculate_importance_score(self, article: dict) -> int:
        """è®¡ç®—æ–‡ç« é‡è¦æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        score = 50  # åŸºç¡€åˆ†
        
        # æ¥æºæƒé‡
        source_weights = {
            'BBCä¸­æ–‡ç½‘': 20, 'BBC World': 20, 'CNNå›½é™…ç‰ˆ': 20,
            'é‡‘èæ—¶æŠ¥ä¸­æ–‡': 18, 'åå°”è¡—æ—¥æŠ¥ä¸­æ–‡': 18,
            'æ¾æ¹ƒæ–°é—»': 15, 'æ–°æµªæ–°é—»': 12, 'ç½‘æ˜“æ–°é—»': 12, 'å‡¤å‡°æ–°é—»': 12,
            'æ—¥ç»äºšæ´²': 15, 'å—åæ—©æŠ¥': 15,
            'ä»Šæ—¥å¤´æ¡çƒ­æ¦œ': 10, 'å¾®åšçƒ­æœ': 8, 'Twitterè¶‹åŠ¿': 8, 'Redditçƒ­é—¨': 8
        }
        
        source = article.get('source', '')
        if source in source_weights:
            score += source_weights[source]
        
        # æ ‡é¢˜å…³é”®è¯åŠ åˆ†
        title = article.get('title', '').lower()
        important_keywords = [
            'çªå‘', 'ç´§æ€¥', 'é‡ç£…', 'ç‹¬å®¶', 'æœ€æ–°', 'é‡å¤§', 'çªç ´', 'é¦–æ¬¡',
            'å±æœº', 'æˆ˜äº‰', 'åœ°éœ‡', 'ç–«æƒ…', 'ç»æµ', 'é‡‘è', 'è‚¡å¸‚', 'æ”¿ç­–',
            'ä¹ è¿‘å¹³', 'æ‹œç™»', 'ç‰¹æœ—æ™®', 'æ™®äº¬'
        ]
        
        for keyword in important_keywords:
            if keyword in title:
                score += 5
        
        # æè¿°é•¿åº¦åŠ åˆ†ï¼ˆå†…å®¹è¶Šè¯¦ç»†å¯èƒ½è¶Šé‡è¦ï¼‰
        description = article.get('description', '')
        if len(description) > 200:
            score += 10
        elif len(description) > 100:
            score += 5
        
        return min(100, max(0, score))  # é™åˆ¶åœ¨0-100ä¹‹é—´
    
    def get_importance_level(self, score: int) -> str:
        """æ ¹æ®åˆ†æ•°è·å–é‡è¦æ€§ç­‰çº§"""
        if score >= 80:
            return "ğŸ”´ éå¸¸é‡è¦"
        elif score >= 65:
            return "ğŸŸ  é‡è¦"
        elif score >= 50:
            return "ğŸŸ¡ ä¸­ç­‰"
        elif score >= 35:
            return "ğŸŸ¢ ä¸€èˆ¬"
        else:
            return "âšª èµ„è®¯"
    
    def parse_publication_time(self, pub_date: str) -> str:
        """è§£æå‘å¸ƒæ—¶é—´"""
        if not pub_date:
            return "æ—¶é—´æœªçŸ¥"
        
        # å°è¯•è§£æå¸¸è§çš„æ—¶é—´æ ¼å¼
        import re
        from datetime import datetime
        
        try:
            # ç§»é™¤æ—¶åŒºä¿¡æ¯
            clean_date = re.sub(r'[+-]\d{2}:?\d{2}$', '', pub_date).strip()
            
            # å°è¯•å¤šç§æ ¼å¼
            formats = [
                '%a, %d %b %Y %H:%M:%S',  # RFC 822æ ¼å¼
                '%Y-%m-%dT%H:%M:%S',      # ISOæ ¼å¼
                '%Y-%m-%d %H:%M:%S',      # æ ‡å‡†æ ¼å¼
                '%d %b %Y %H:%M:%S',      # ç®€å†™æœˆä»½æ ¼å¼
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(clean_date, fmt)
                    return dt.strftime('%m-%d %H:%M')
                except ValueError:
                    continue
            
            # å¦‚æœéƒ½æ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²ï¼ˆæˆªæ–­ï¼‰
            return pub_date[:16]
            
        except Exception:
            return "æ—¶é—´è§£æé”™è¯¯"
    
    def get_time_recency(self, time_str: str) -> str:
        """è·å–æ—¶é—´æ–°é²œåº¦"""
        if "æ—¶é—´æœªçŸ¥" in time_str or "è§£æé”™è¯¯" in time_str:
            return "ğŸ•’ æ—¶é—´æœªçŸ¥"
        
        try:
            from datetime import datetime
            
            # å°è¯•è§£ææ—¶é—´
            now = datetime.now()
            time_format = '%m-%d %H:%M'
            
            try:
                article_time = datetime.strptime(time_str, time_format)
                # è®¾ç½®å¹´ä»½ä¸ºå½“å‰å¹´ä»½
                article_time = article_time.replace(year=now.year)
                
                # è®¡ç®—æ—¶é—´å·®
                time_diff = now - article_time
                hours_diff = time_diff.total_seconds() / 3600
                
                if hours_diff < 1:
                    return "ğŸ†• åˆšåˆšæ›´æ–°"
                elif hours_diff < 3:
                    return "ğŸ†• 3å°æ—¶å†…"
                elif hours_diff < 12:
                    return "ğŸ•’ åŠå¤©å†…"
                elif hours_diff < 24:
                    return "ğŸ•’ ä»Šå¤©"
                elif hours_diff < 48:
                    return "ğŸ•’ æ˜¨å¤©"
                else:
                    days = int(hours_diff / 24)
                    return f"ğŸ•’ {days}å¤©å‰"
                    
            except ValueError:
                return "ğŸ•’ " + time_str
                
        except Exception:
            return "ğŸ•’ " + time_str
    
    def format_stock_section(self, stocks_data: list) -> str:
        """æ ¼å¼åŒ–è‚¡ç¥¨éƒ¨åˆ†"""
        if not stocks_data:
            return "ğŸ“­ æš‚æ—¶æ— æ³•è·å–è‚¡ç¥¨æ•°æ®\n"
        
        section = "ğŸ“ˆ **è‚¡ç¥¨ç›‘æ§**\n\n"
        
        for stock in stocks_data:
            sentiment = self.analyze_stock_sentiment(stock['change_percent'])
            
            section += f"â€¢ **{stock['name']}** ({stock['symbol']})\n"
            section += f"  ä»·æ ¼: {stock['price']:.2f} {stock['currency']}\n"
            section += f"  æ¶¨è·Œ: {stock['change']:+.2f} ({stock['change_percent']:+.2f}%)\n"
            section += f"  æƒ…ç»ª: {sentiment}\n\n"
        
        return section
    
    def format_news_section(self, articles: list) -> str:
        """æ ¼å¼åŒ–æ–°é—»éƒ¨åˆ†ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰"""
        if not articles:
            return "ğŸ“­ æš‚æ—¶æ²¡æœ‰æ–°æ–°é—»\n"
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = {}
        for article in articles:
            category = article.get('category', 'å…¶ä»–')
            if category not in categories:
                categories[category] = []
            categories[category].append(article)
        
        section = "ğŸ“° **é‡è¦æ–°é—»**\n\n"
        
        # é€‰æ‹©æœ€é‡è¦çš„æ–°é—»ï¼ˆæ¯ç±»åˆ«æœ€å¤š3æ¡ï¼Œæ€»å…±æœ€å¤š8æ¡ï¼‰
        selected_articles = []
        for category, cat_articles in categories.items():
            # æ¯ç±»åˆ«å–æœ€é‡è¦çš„2-3æ¡
            cat_selected = cat_articles[:3]
            selected_articles.extend(cat_selected)
        
        # å¦‚æœæ€»æ•°å¤ªå¤šï¼Œé™åˆ¶ä¸º8æ¡
        selected_articles = selected_articles[:8]
        
        # æŒ‰ç±»åˆ«æ˜¾ç¤º
        displayed_categories = set()
        article_counter = 1
        
        for article in selected_articles:
            category = article.get('category', 'å…¶ä»–')
            
            # å¦‚æœæ˜¯æ–°çš„ç±»åˆ«ï¼Œæ·»åŠ ç±»åˆ«æ ‡é¢˜
            if category not in displayed_categories:
                # æ·»åŠ ç±»åˆ«è¡¨æƒ…
                category_emoji = {
                    'å›½å†…åª’ä½“': 'ğŸ‡¨ğŸ‡³',
                    'å›½é™…åª’ä½“': 'ğŸŒ',
                    'å›½é™…è´¢ç»': 'ğŸ’¹',
                    'äºšæ´²åª’ä½“': 'ğŸŒ',
                    'ç¤¾äº¤åª’ä½“': 'ğŸ’¬',
                    'å…¶ä»–': 'ğŸ“'
                }.get(category, 'ğŸ“°')
                
                section += f"{category_emoji} **{category}**\n"
                displayed_categories.add(category)
            
            # æ ¼å¼åŒ–å•æ¡æ–°é—»ï¼ˆä½¿ç”¨å¢å¼ºä¿¡æ¯ï¼‰
            title = article['title'][:100]  # é™åˆ¶æ ‡é¢˜é•¿åº¦
            url = article.get('url', '')
            
            # ç”Ÿæˆè¯¦ç»†æ‘˜è¦
            description = article.get('description', '')
            summary = self.generate_summary(description)
            
            # å¢å¼ºæ–‡ç« ä¿¡æ¯
            enhanced_article = self.enhance_article_info(article)
            extra_info = enhanced_article.get('extra_info', '')
            read_time = enhanced_article.get('read_time', '')
            importance = enhanced_article.get('importance', 'âšª èµ„è®¯')
            update_time = enhanced_article.get('update_time', 'æ—¶é—´æœªçŸ¥')
            time_recency = enhanced_article.get('time_recency', 'ğŸ•’ æ—¶é—´æœªçŸ¥')
            
            source = article['source']
            
            # æ·»åŠ æ¥æºè¡¨æƒ…
            source_emoji = {
                'BBCä¸­æ–‡ç½‘': 'ğŸ‡¬ğŸ‡§',
                'BBC World': 'ğŸ‡¬ğŸ‡§',
                'CNNå›½é™…ç‰ˆ': 'ğŸ‡ºğŸ‡¸',
                'é‡‘èæ—¶æŠ¥ä¸­æ–‡': 'ğŸ’·',
                'æ—¥ç»äºšæ´²': 'ğŸ‡¯ğŸ‡µ',
                'å—åæ—©æŠ¥': 'ğŸ‡­ğŸ‡°',
                'æ–°æµªæ–°é—»': 'ğŸ¦Š',
                'ç½‘æ˜“æ–°é—»': 'ğŸ¦Œ',
                'å‡¤å‡°æ–°é—»': 'ğŸ¦š',
                'æ¾æ¹ƒæ–°é—»': 'ğŸŒŠ',
                'ä»Šæ—¥å¤´æ¡çƒ­æ¦œ': 'ğŸ“±',
                'å¾®åšçƒ­æœ': 'ğŸ¦',
                'Twitterè¶‹åŠ¿': 'ğŸ¦',
                'Redditçƒ­é—¨': 'ğŸ‘¾'
            }.get(source, 'ğŸ“°')
            
            section += f"  {article_counter}. **{title}**\n"
            
            # ç¬¬ä¸€è¡Œï¼šé‡è¦æ€§ + æ¥æº + æ›´æ–°æ—¶é—´
            section += f"     {importance} | {source_emoji} {source} | {time_recency}\n"
            
            # ç¬¬äºŒè¡Œï¼šå…·ä½“æ›´æ–°æ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if update_time != "æ—¶é—´æœªçŸ¥" and "è§£æé”™è¯¯" not in update_time:
                section += f"     æ›´æ–°æ—¶é—´: {update_time}\n"
            
            # ç¬¬ä¸‰è¡Œï¼šé¢å¤–ä¿¡æ¯æ ‡ç­¾
            if extra_info:
                section += f"     {extra_info}\n"
            
            # ç¬¬å››è¡Œï¼šè®¿é—®é“¾æ¥
            if url and url.startswith('http'):
                section += f"     ğŸ”— {url}\n"
            
            # ç¬¬äº”è¡Œï¼šè¯¦ç»†æ‘˜è¦
            if summary and summary != "æš‚æ— è¯¦ç»†å†…å®¹æ‘˜è¦":
                section += f"     ğŸ“ **æ‘˜è¦**: {summary}\n"
            
            # ç¬¬å…­è¡Œï¼šé˜…è¯»æ—¶é—´
            if read_time:
                section += f"     {read_time}\n"
            
            section += "\n"
            
            article_counter += 1
        
        # æ ‡è®°ä¸ºå·²æ¨é€
        for article in selected_articles:
            self.mark_article_pushed(article)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯å’Œè®¿é—®æç¤º
        section += f"ğŸ“Š æœ¬æ¬¡æ¨é€: {len(selected_articles)}æ¡æ–°é—»ï¼Œæ¥è‡ª{len(displayed_categories)}ä¸ªç±»åˆ«\n"
        section += f"ğŸ’¡ æç¤º: ç‚¹å‡»è“è‰²é“¾æ¥å¯ç›´æ¥è®¿é—®æ–°é—»åŸæ–‡\n"
        
        return section
    
    def format_price_alerts(self, stocks_data: list) -> str:
        """æ ¼å¼åŒ–ä»·æ ¼é¢„è­¦"""
        alerts = []
        
        # é¢„è­¦é˜ˆå€¼é…ç½®
        alert_thresholds = {
            "é˜¿é‡Œå·´å·´-W": {"above": 165.0, "below": 158.0},
            "å°ç±³é›†å›¢-W": {"above": 35.0, "below": 34.0},
            "æ¯”äºšè¿ª": {"above": 88.0, "below": 86.0}
        }
        
        for stock in stocks_data:
            name = stock['name']
            price = stock['price']
            
            if name in alert_thresholds:
                thresholds = alert_thresholds[name]
                
                if price > thresholds["above"]:
                    alerts.append(f"âš ï¸ {name} çªç ´ {thresholds['above']} {stock['currency']}")
                elif price < thresholds["below"]:
                    alerts.append(f"âš ï¸ {name} è·Œç ´ {thresholds['below']} {stock['currency']}")
                
                # æ¶¨è·Œå¹…è¶…è¿‡3%
                if abs(stock['change_percent']) > 3:
                    alerts.append(f"âš ï¸ {name} æ¶¨è·Œå¹…è¶…è¿‡3% ({stock['change_percent']:+.2f}%)")
        
        if alerts:
            section = "âš ï¸ **ä»·æ ¼é¢„è­¦**\n\n"
            for alert in alerts:
                section += f"â€¢ {alert}\n"
            section += "\n"
            return section
        
        return ""
    
    def generate_full_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%H:%M')
        
        report = f"ğŸ“Š **æ–°é—»+è‚¡ç¥¨æ¨é€** ({timestamp})\n\n"
        
        # è·å–è‚¡ç¥¨æ•°æ®
        stocks_data = self.get_all_stocks_data()
        report += self.format_stock_section(stocks_data)
        
        # è·å–æ–°é—»
        all_news = self.fetch_all_news()
        new_articles = self.filter_new_articles(all_news)
        report += self.format_news_section(new_articles)
        
        # ä»·æ ¼é¢„è­¦
        if stocks_data:
            alerts_section = self.format_price_alerts(stocks_data)
            if alerts_section:
                report += alerts_section
        
        # ç»Ÿè®¡ä¿¡æ¯
        report += "---\n"
        report += f"ğŸ“Š **ç»Ÿè®¡ä¿¡æ¯**\n"
        report += f"â€¢ ç›‘æ§è‚¡ç¥¨: {len(self.stocks)} åª\n"
        report += f"â€¢ æ–°é—»æ¥æº: {len(self.news_sources)} ä¸ªï¼ˆå›½å†…{sum(1 for s in self.news_sources if 'å›½å†…' in s['category'])}ä¸ªï¼Œå›½é™…{sum(1 for s in self.news_sources if 'å›½é™…' in s['category'] or 'äºšæ´²' in s['category'])}ä¸ªï¼‰\n"
        report += f"â€¢ æ–°æ–‡ç« æ•°: {len(new_articles)} æ¡\n"
        
        # æ–°é—»åˆ†ç±»ç»Ÿè®¡
        if new_articles:
            categories = {}
            for article in new_articles:
                category = article.get('category', 'å…¶ä»–')
                categories[category] = categories.get(category, 0) + 1
            
            report += f"â€¢ æ–°é—»åˆ†ç±»: "
            cat_list = []
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:3]:
                cat_list.append(f"{category}({count})")
            report += ", ".join(cat_list) + "\n"
        
        if stocks_data:
            up_count = sum(1 for s in stocks_data if s['change_percent'] > 0)
            down_count = sum(1 for s in stocks_data if s['change_percent'] < 0)
            report += f"â€¢ è‚¡ç¥¨æ¶¨è·Œ: {up_count}æ¶¨ {down_count}è·Œ\n"
        
        report += f"\nğŸ”„ ä¸‹æ¬¡æ¨é€: {(datetime.now() + timedelta(hours=1)).strftime('%H:%M')}\n"
        report += f"ğŸ“± æ¥æ”¶æ–¹å¼: WhatsApp\n"
        report += f"â° æ¨é€é¢‘ç‡: æ¯å°æ—¶ä¸€æ¬¡\n"
        
        return report
    
    def save_report(self, report: str):
        """ä¿å­˜æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        report_file = f"/home/admin/clawd/push_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file
    
    def run(self):
        """è¿è¡Œæ¨é€ç³»ç»Ÿ"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ æ–°é—»+è‚¡ç¥¨æ¨é€ç³»ç»Ÿå¯åŠ¨")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}")
        
        try:
            # æ¸…ç†7å¤©å‰çš„æ—§è®°å½•
            self.cleanup_old_records(days_to_keep=7)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_full_report()
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = self.save_report(report)
            
            print(f"\nâœ… æ¨é€æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
            print(f"   æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
            print(f"   ä¿å­˜ä½ç½®: {report_file}")
            
            # æ˜¾ç¤ºé¢„è§ˆ
            preview = report[:300] + "..." if len(report) > 300 else report
            print(f"\nğŸ“„ æŠ¥å‘Šé¢„è§ˆ:")
            print("-"*40)
            print(preview)
            print("-"*40)
            
            return report
            
        except Exception as e:
            print(f"âŒ æ¨é€ç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    pusher = NewsStockPusher()
    report = pusher.run()
    
    if report:
        print(f"\n{'='*60}")
        print("âœ… æ¨é€ç³»ç»Ÿè¿è¡ŒæˆåŠŸ!")
        print("ğŸ“¤ è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å‘é€åˆ°WhatsApp:")
        print(f"   openclaw message send -t +8618966719971 -m 'æŠ¥å‘Šå†…å®¹'")
        print(f"{'='*60}")
        return True
    else:
        print(f"\n{'='*60}")
        print("âŒ æ¨é€ç³»ç»Ÿè¿è¡Œå¤±è´¥")
        print(f"{'='*60}")
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)