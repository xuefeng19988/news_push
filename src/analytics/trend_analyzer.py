#!/usr/bin/env python3
"""
è¶‹åŠ¿åˆ†æå™¨
åˆ†ææ–°é—»è¶‹åŠ¿ã€å…³é”®è¯æå–ã€æƒ…æ„Ÿåˆ†æ
"""

import re
import json
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import math

class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨"""
    
    def __init__(self, db_path: str = "./news_cache.db"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = db_path
        
        # ä¸­æ–‡åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.chinese_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦',
            'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'å¥¹', 'ä»–', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬',
            'å®ƒä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™äº›', 'é‚£äº›', 'è¿™é‡Œ', 'é‚£é‡Œ', 'è¿™æ ·', 'é‚£æ ·', 'è¿™ä¹ˆ', 'é‚£ä¹ˆ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ',
            'å¯ä»¥', 'å¯èƒ½', 'å¯èƒ½', 'èƒ½å¤Ÿ', 'éœ€è¦', 'åº”è¯¥', 'å¿…é¡»', 'ä¸€å®š', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'å¤§çº¦', 'å·¦å³', 'ä¸Šä¸‹', 'å‰å'
        }
        
        # æƒ…æ„Ÿè¯å…¸ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.sentiment_lexicon = {
            'positive': {
                'å¥½', 'ä¼˜ç§€', 'æˆåŠŸ', 'èƒœåˆ©', 'è¿›æ­¥', 'å‘å±•', 'å¢é•¿', 'æé«˜', 'æå‡', 'æ”¹å–„', 'ä¼˜åŒ–', 'åˆ›æ–°', 'çªç ´', 'é¢†å…ˆ',
                'å…ˆè¿›', 'å¼ºå¤§', 'ç¹è£', 'ç¨³å®š', 'å®‰å…¨', 'å¯é ', 'ä¿¡ä»»', 'æ»¡æ„', 'é«˜å…´', 'å¿«ä¹', 'å¹¸ç¦', 'çˆ±', 'å–œæ¬¢', 'æ”¯æŒ',
                'èµæˆ', 'åŒæ„', 'è®¤å¯', 'è¡¨æ‰¬', 'èµç¾', 'é¼“åŠ±', 'å¸®åŠ©', 'åˆä½œ', 'å…±èµ¢', 'æˆåŠŸ', 'æˆå°±', 'è¾‰ç…Œ', 'å…‰æ˜', 'å¸Œæœ›',
                'æœªæ¥', 'å‰æ™¯', 'æœºä¼š', 'æœºé‡', 'æ½œåŠ›', 'ä»·å€¼', 'æ„ä¹‰', 'é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'ä¸­å¿ƒ', 'ä¸»è¦', 'é¦–è¦', 'å¿…è¦'
            },
            'negative': {
                'å', 'ç³Ÿç³•', 'å¤±è´¥', 'å¤±åˆ©', 'é€€æ­¥', 'è¡°é€€', 'ä¸‹é™', 'é™ä½', 'ä¸‹è·Œ', 'æ¶åŒ–', 'æ¶åŒ–', 'è½å', 'è½å', 'å¼±å°',
                'è´«ç©·', 'åŠ¨è¡', 'å±é™©', 'ä¸å¯é ', 'ä¸ä¿¡ä»»', 'ä¸æ»¡æ„', 'ä¸é«˜å…´', 'æ‚²ä¼¤', 'ç—›è‹¦', 'æ¨', 'è®¨åŒ', 'åå¯¹', 'å¦å†³',
                'æ‹’ç»', 'å¦è®¤', 'æ‰¹è¯„', 'æŒ‡è´£', 'æŠ±æ€¨', 'å¦¨ç¢', 'ç ´å', 'å†²çª', 'çŸ›ç›¾', 'é—®é¢˜', 'å›°éš¾', 'æŒ‘æˆ˜', 'é£é™©', 'å±æœº',
                'å¨èƒ', 'å‹åŠ›', 'ç´§å¼ ', 'ç„¦è™‘', 'ææƒ§', 'æ‹…å¿§', 'å¤±æœ›', 'ç»æœ›', 'é»‘æš—', 'è¿‡å»', 'å†å²', 'æ•™è®­', 'æŸå¤±', 'æŸå®³',
                'ä¼¤å®³', 'ç ´å', 'æ¯ç­', 'ç¾éš¾', 'äº‹æ•…', 'é”™è¯¯', 'å¤±è¯¯', 'ç¼ºç‚¹', 'ä¸è¶³', 'ç¼ºé™·', 'æ¼æ´', 'å¼±ç‚¹', 'çŸ­æ¿'
            }
        }
        
        # é¢†åŸŸå…³é”®è¯
        self.domain_keywords = {
            'æ”¿æ²»': ['æ”¿åºœ', 'æ”¿æ²»', 'å¤–äº¤', 'å›½é™…', 'å›½å®¶', 'ä¸»å¸­', 'æ€»ç»Ÿ', 'æ€»ç†', 'å›½ä¼š', 'è®®ä¼š', 'é€‰ä¸¾', 'æ”¿ç­–', 'æ³•å¾‹', 'æ³•è§„'],
            'ç»æµ': ['ç»æµ', 'GDP', 'è´¢æ”¿', 'é‡‘è', 'è´§å¸', 'é“¶è¡Œ', 'æŠ•èµ„', 'å¸‚åœº', 'è´¸æ˜“', 'å•†ä¸š', 'ä¼ä¸š', 'å…¬å¸', 'äº§ä¸š', 'è¡Œä¸š'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'æŠ€æœ¯', 'åˆ›æ–°', 'ç ”å‘', 'ç§‘å­¦', 'ç ”ç©¶', 'å¼€å‘', 'AI', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'äº’è”ç½‘', 'æ•°å­—'],
            'è‚¡ç¥¨': ['è‚¡ç¥¨', 'è‚¡å¸‚', 'è‚¡ä»·', 'æŒ‡æ•°', 'æŠ•èµ„', 'è¯åˆ¸', 'äº¤æ˜“æ‰€', 'å¸‚å€¼', 'æ¶¨è·Œ', 'äº¤æ˜“', 'ä¹°å–', 'ç‰›å¸‚', 'ç†Šå¸‚'],
            'å›½é™…': ['å›½é™…', 'å…¨çƒ', 'ä¸–ç•Œ', 'å›½å®¶', 'åœ°åŒº', 'å¤§é™†', 'æµ·å¤–', 'å¤–å›½', 'è·¨å›½', 'è·¨å¢ƒ', 'å¤–äº¤', 'å…³ç³»', 'åˆä½œ', 'ç«äº‰'],
            'ç¤¾ä¼š': ['ç¤¾ä¼š', 'æ°‘ç”Ÿ', 'äººæ°‘', 'ç¾¤ä¼—', 'å…¬ä¼—', 'å…¬æ°‘', 'ç”Ÿæ´»', 'å·¥ä½œ', 'å°±ä¸š', 'æ•™è‚²', 'åŒ»ç–—', 'å¥åº·', 'ç¯å¢ƒ', 'å®‰å…¨']
        }
    
    def extract_keywords(self, text: str, top_n: int = 10) -> List[Dict[str, Any]]:
        """
        æå–å…³é”®è¯ï¼ˆåŸºäºTF-IDFç®€åŒ–ç‰ˆï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…³é”®è¯åŒ…å«è¯ã€é¢‘ç‡ã€TF-IDFåˆ†æ•°
        """
        if not text or len(text) < 10:
            return []
        
        # ä¸­æ–‡åˆ†è¯ï¼ˆç®€åŒ–ç‰ˆï¼šæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼åˆ†å‰²ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
        words = [word for word in words if word not in self.chinese_stopwords]
        
        if not words:
            return []
        
        # è®¡ç®—è¯é¢‘
        word_counts = Counter(words)
        total_words = len(words)
        
        # è®¡ç®—TFï¼ˆè¯é¢‘ï¼‰
        keywords = []
        for word, count in word_counts.most_common(top_n * 2):  # å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤
            tf = count / total_words
            
            # ç®€å•IDFä¼°è®¡ï¼ˆåŸºäºé¢†åŸŸè¯å…¸ï¼‰
            idf = 1.0
            for domain, domain_words in self.domain_keywords.items():
                if word in domain_words:
                    idf = 2.0  # é¢†åŸŸå…³é”®è¯æƒé‡æ›´é«˜
                    break
            
            tfidf = tf * idf
            
            # è¿‡æ»¤å¤ªå¸¸è§çš„è¯
            if tf < 0.01:  # é¢‘ç‡å¤ªä½ï¼Œå¯èƒ½ä¸é‡è¦
                continue
                
            keywords.append({
                'word': word,
                'frequency': count,
                'tf': round(tf, 4),
                'idf': idf,
                'tfidf': round(tfidf, 4),
                'domain': self._identify_domain(word)
            })
        
        # æŒ‰TF-IDFæ’åº
        keywords.sort(key=lambda x: x['tfidf'], reverse=True)
        return keywords[:top_n]
    
    def _identify_domain(self, word: str) -> str:
        """è¯†åˆ«è¯æ‰€å±é¢†åŸŸ"""
        for domain, keywords in self.domain_keywords.items():
            if word in keywords:
                return domain
        return 'å…¶ä»–'
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            æƒ…æ„Ÿåˆ†æç»“æœ
        """
        if not text:
            return {'sentiment': 'neutral', 'score': 0.0, 'positive_words': [], 'negative_words': []}
        
        # æå–ä¸­æ–‡è¯è¯­
        words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
        
        positive_words = []
        negative_words = []
        positive_count = 0
        negative_count = 0
        
        for word in words:
            if word in self.sentiment_lexicon['positive']:
                positive_words.append(word)
                positive_count += 1
            elif word in self.sentiment_lexicon['negative']:
                negative_words.append(word)
                negative_count += 1
        
        total_emotional_words = positive_count + negative_count
        
        if total_emotional_words == 0:
            return {
                'sentiment': 'neutral',
                'score': 0.0,
                'positive_words': [],
                'negative_words': [],
                'positive_count': 0,
                'negative_count': 0,
                'total_emotional_words': 0
            }
        
        sentiment_score = (positive_count - negative_count) / total_emotional_words
        
        if sentiment_score > 0.2:
            sentiment = 'positive'
        elif sentiment_score < -0.2:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'sentiment': sentiment,
            'score': round(sentiment_score, 3),
            'positive_words': list(set(positive_words)),
            'negative_words': list(set(negative_words)),
            'positive_count': positive_count,
            'negative_count': negative_count,
            'total_emotional_words': total_emotional_words
        }
    
    def analyze_news_trends(self, articles: List[Dict[str, Any]], hours: int = 24) -> Dict[str, Any]:
        """
        åˆ†ææ–°é—»è¶‹åŠ¿
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            hours: åˆ†æçš„æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        if not articles:
            return {'error': 'æ²¡æœ‰æ–‡ç« æ•°æ®'}
        
        # æŒ‰æ—¶é—´åˆ†ç»„ï¼ˆç®€åŒ–ï¼šæŒ‰æœ€è¿‘Nå°æ—¶ï¼‰
        now = datetime.now()
        cutoff_time = now - timedelta(hours=hours)
        
        recent_articles = []
        for article in articles:
            try:
                if isinstance(article.get('published'), str):
                    # å°è¯•è§£ææ—¶é—´
                    pub_time = datetime.fromisoformat(article['published'].replace('Z', '+00:00'))
                else:
                    pub_time = now
                
                if pub_time > cutoff_time:
                    recent_articles.append(article)
            except:
                recent_articles.append(article)
        
        if not recent_articles:
            return {'error': f'æœ€è¿‘{hours}å°æ—¶å†…æ²¡æœ‰æ–‡ç« '}
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬å†…å®¹
        all_text = ' '.join([
            f"{article.get('title', '')} {article.get('summary', '')}"
            for article in recent_articles
        ])
        
        # æå–å…³é”®è¯
        keywords = self.extract_keywords(all_text, top_n=20)
        
        # åˆ†æé¢†åŸŸåˆ†å¸ƒ
        domain_distribution = {}
        for article in recent_articles:
            article_type = article.get('type', 'ä¸€èˆ¬æ–°é—»')
            if 'ã€' in article_type:
                types = article_type.split('ã€')
                for t in types:
                    domain_distribution[t] = domain_distribution.get(t, 0) + 1
            else:
                domain_distribution[article_type] = domain_distribution.get(article_type, 0) + 1
        
        # åˆ†æé‡è¦æ€§åˆ†å¸ƒ
        importance_distribution = {}
        for article in recent_articles:
            importance = article.get('importance', 'ä¸­')
            importance_distribution[importance] = importance_distribution.get(importance, 0) + 1
        
        # åˆ†ææ¥æºåˆ†å¸ƒ
        source_distribution = {}
        for article in recent_articles:
            source = article.get('source', 'æœªçŸ¥')
            source_distribution[source] = source_distribution.get(source, 0) + 1
        
        # æƒ…æ„Ÿåˆ†æ
        sentiment_results = []
        for article in recent_articles[:10]:  # åªåˆ†æå‰10ç¯‡
            text = f"{article.get('title', '')} {article.get('summary', '')}"
            sentiment = self.analyze_sentiment(text)
            sentiment_results.append({
                'title': article.get('title', '')[:50],
                'sentiment': sentiment['sentiment'],
                'score': sentiment['score']
            })
        
        # è®¡ç®—æ€»ä½“æƒ…æ„Ÿ
        total_score = sum(s.get('score', 0) for s in sentiment_results)
        avg_sentiment_score = total_score / len(sentiment_results) if sentiment_results else 0
        
        if avg_sentiment_score > 0.1:
            overall_sentiment = 'positive'
        elif avg_sentiment_score < -0.1:
            overall_sentiment = 'negative'
        else:
            overall_sentiment = 'neutral'
        
        return {
            'period': f'æœ€è¿‘{hours}å°æ—¶',
            'total_articles': len(recent_articles),
            'timestamp': now.isoformat(),
            
            # å…³é”®è¯åˆ†æ
            'top_keywords': keywords[:10],
            
            # åˆ†å¸ƒåˆ†æ
            'domain_distribution': [
                {'domain': domain, 'count': count}
                for domain, count in sorted(domain_distribution.items(), key=lambda x: x[1], reverse=True)
            ],
            'importance_distribution': [
                {'importance': imp, 'count': count}
                for imp, count in sorted(importance_distribution.items(), key=lambda x: x[1], reverse=True)
            ],
            'source_distribution': [
                {'source': src, 'count': count}
                for src, count in sorted(source_distribution.items(), key=lambda x: x[1], reverse=True)
            ],
            
            # æƒ…æ„Ÿåˆ†æ
            'sentiment_analysis': {
                'overall_sentiment': overall_sentiment,
                'average_score': round(avg_sentiment_score, 3),
                'sample_articles': sentiment_results[:5],
                'positive_count': sum(1 for s in sentiment_results if s['sentiment'] == 'positive'),
                'negative_count': sum(1 for s in sentiment_results if s['sentiment'] == 'negative'),
                'neutral_count': sum(1 for s in sentiment_results if s['sentiment'] == 'neutral')
            },
            
            # è¶‹åŠ¿æ´å¯Ÿ
            'insights': self._generate_insights(
                keywords,
                domain_distribution,
                importance_distribution,
                sentiment_results
            )
        }
    
    def _generate_insights(self, keywords, domain_distribution, importance_distribution, sentiment_results) -> List[str]:
        """ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ"""
        insights = []
        
        # åŸºäºå…³é”®è¯çš„æ´å¯Ÿ
        if keywords:
            top_keyword = keywords[0]['word']
            insights.append(f"çƒ­é—¨è¯é¢˜: '{top_keyword}' æ˜¯å½“å‰æœ€å—å…³æ³¨çš„è¯é¢˜")
        
        # åŸºäºé¢†åŸŸçš„æ´å¯Ÿ
        if domain_distribution:
            top_domain = max(domain_distribution.items(), key=lambda x: x[1])
            insights.append(f"ä¸»è¦é¢†åŸŸ: {top_domain[0]}ç±»æ–°é—»å æ¯”æœ€é«˜ ({top_domain[1]}ç¯‡)")
        
        # åŸºäºé‡è¦æ€§çš„æ´å¯Ÿ
        if importance_distribution:
            high_importance = importance_distribution.get('é«˜', 0)
            if high_importance > 0:
                insights.append(f"é‡è¦æ–°é—»: æœ‰ {high_importance} ç¯‡é«˜é‡è¦æ€§æ–°é—»ï¼Œå€¼å¾—ç‰¹åˆ«å…³æ³¨")
        
        # åŸºäºæƒ…æ„Ÿçš„æ´å¯Ÿ
        if sentiment_results:
            positive_count = sum(1 for s in sentiment_results if s['sentiment'] == 'positive')
            negative_count = sum(1 for s in sentiment_results if s['sentiment'] == 'negative')
            
            if positive_count > negative_count * 2:
                insights.append("æƒ…æ„Ÿå€¾å‘: å½“å‰æ–°é—»æ€»ä½“åæ­£é¢")
            elif negative_count > positive_count * 2:
                insights.append("æƒ…æ„Ÿå€¾å‘: å½“å‰æ–°é—»æ€»ä½“åè´Ÿé¢")
            else:
                insights.append("æƒ…æ„Ÿå€¾å‘: å½“å‰æ–°é—»æƒ…æ„Ÿåˆ†å¸ƒè¾ƒä¸ºå¹³è¡¡")
        
        # ç»„åˆæ´å¯Ÿ
        if len(insights) < 3:
            insights.append("è¶‹åŠ¿è§‚å¯Ÿ: æ–°é—»åˆ†å¸ƒè¾ƒä¸ºå‡è¡¡ï¼Œæ²¡æœ‰æ˜æ˜¾çƒ­ç‚¹")
        
        return insights[:5]  # æœ€å¤š5æ¡æ´å¯Ÿ
    
    def analyze_stock_correlation(self, news_trends: Dict[str, Any], stock_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†ææ–°é—»ä¸è‚¡ç¥¨ç›¸å…³æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            news_trends: æ–°é—»è¶‹åŠ¿åˆ†æç»“æœ
            stock_data: è‚¡ç¥¨æ•°æ®åˆ—è¡¨
            
        Returns:
            ç›¸å…³æ€§åˆ†æç»“æœ
        """
        if not stock_data:
            return {'error': 'æ²¡æœ‰è‚¡ç¥¨æ•°æ®'}
        
        # ç®€åŒ–çš„ç›¸å…³æ€§åˆ†æ
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨æ—¶é—´åºåˆ—åˆ†æå’Œç›¸å…³æ€§è®¡ç®—
        
        correlation_results = []
        
        for stock in stock_data:
            stock_name = stock.get('name', 'æœªçŸ¥')
            stock_symbol = stock.get('symbol', '')
            stock_change = stock.get('change_percent', 0)
            
            # ç®€å•çš„ç›¸å…³æ€§é€»è¾‘ï¼ˆç¤ºä¾‹ï¼‰
            correlation = 'unknown'
            correlation_score = 0
            
            if news_trends.get('overall_sentiment') == 'positive' and stock_change > 0:
                correlation = 'positive'
                correlation_score = 0.7
            elif news_trends.get('overall_sentiment') == 'negative' and stock_change < 0:
                correlation = 'negative'
                correlation_score = 0.7
            elif abs(stock_change) < 0.5:
                correlation = 'neutral'
                correlation_score = 0.3
            
            correlation_results.append({
                'stock': stock_name,
                'symbol': stock_symbol,
                'change_percent': stock_change,
                'correlation': correlation,
                'correlation_score': correlation_score,
                'news_impact': self._estimate_news_impact(stock_name, news_trends)
            })
        
        return {
            'analysis_period': news_trends.get('period', 'æœªçŸ¥'),
            'overall_sentiment': news_trends.get('sentiment_analysis', {}).get('overall_sentiment', 'unknown'),
            'stock_correlations': correlation_results,
            'insights': [
                f"æ–°é—»æƒ…æ„Ÿä¸è‚¡ç¥¨æ¶¨è·Œçš„ç®€å•ç›¸å…³æ€§åˆ†æ",
                f"åŸºäºæœ€è¿‘æ–°é—»è¶‹åŠ¿å¯¹è‚¡ç¥¨è¡¨ç°çš„åˆæ­¥è¯„ä¼°"
            ]
        }
    
    def _estimate_news_impact(self, stock_name: str, news_trends: Dict[str, Any]) -> str:
        """ä¼°è®¡æ–°é—»å¯¹è‚¡ç¥¨çš„å½±å“ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åŸºäºè‚¡ç¥¨åç§°å’Œæ–°é—»å…³é”®è¯çš„ç®€å•åŒ¹é…
        stock_keywords = {
            'é˜¿é‡Œå·´å·´': ['ç”µå•†', 'ç§‘æŠ€', 'äº’è”ç½‘', 'é©¬äº‘', 'æ·˜å®', 'å¤©çŒ«'],
            'å°ç±³': ['æ‰‹æœº', 'ç§‘æŠ€', 'æ™ºèƒ½', 'é›·å†›', 'ç¡¬ä»¶'],
            'æ¯”äºšè¿ª': ['æ±½è½¦', 'æ–°èƒ½æº', 'ç”µåŠ¨è½¦', 'ç”µæ± ', 'åˆ¶é€ ']
        }
        
        keywords = [kw['word'] for kw in news_trends.get('top_keywords', [])]
        
        for stock, stock_kws in stock_keywords.items():
            if stock in stock_name:
                matching_keywords = [kw for kw in keywords if kw in stock_kws]
                if matching_keywords:
                    return f"é«˜ç›¸å…³ï¼ˆç›¸å…³å…³é”®è¯: {', '.join(matching_keywords[:3])}ï¼‰"
        
        return "ä¸€èˆ¬ç›¸å…³"


def test_trend_analyzer():
    """æµ‹è¯•è¶‹åŠ¿åˆ†æå™¨"""
    print("ğŸ§ª æµ‹è¯•è¶‹åŠ¿åˆ†æå™¨")
    print("=" * 60)
    
    analyzer = TrendAnalyzer()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    ä¸­å›½æ”¿åºœå®£å¸ƒæ–°çš„ç»æµåˆºæ¿€è®¡åˆ’ï¼Œæ—¨åœ¨ä¿ƒè¿›ç»æµå¢é•¿å’Œåˆ›é€ å°±ä¸šã€‚
    è¿™ä¸€ä¸¾æªå—åˆ°äº†å¸‚åœºçš„ç§¯æå›åº”ï¼Œè‚¡å¸‚ä»Šå¤©å¤§å¹…ä¸Šæ¶¨ã€‚
    ä¸“å®¶è®¤ä¸ºè¿™ä¸€æ”¿ç­–å°†æœ‰åŠ©äºç¨³å®šç»æµå’Œå¢å¼ºå¸‚åœºä¿¡å¿ƒã€‚
    """
    
    print("ğŸ“Š æµ‹è¯•å…³é”®è¯æå–:")
    keywords = analyzer.extract_keywords(test_text, top_n=5)
    for kw in keywords:
        print(f"  â€¢ {kw['word']} (é¢‘ç‡: {kw['frequency']}, TF-IDF: {kw['tfidf']}, é¢†åŸŸ: {kw['domain']})")
    
    print("\nğŸ“Š æµ‹è¯•æƒ…æ„Ÿåˆ†æ:")
    sentiment = analyzer.analyze_sentiment(test_text)
    print(f"  æƒ…æ„Ÿ: {sentiment['sentiment']}")
    print(f"  åˆ†æ•°: {sentiment['score']}")
    print(f"  ç§¯æè¯: {', '.join(sentiment['positive_words'][:5])}")
    print(f"  æ¶ˆæè¯: {', '.join(sentiment['negative_words'][:5])}")
    
    print("\nğŸ“Š æµ‹è¯•æ–°é—»è¶‹åŠ¿åˆ†æ:")
    test_articles = [
        {
            'title': 'ä¸­å›½ç»æµåˆºæ¿€è®¡åˆ’æ¨åŠ¨è‚¡å¸‚ä¸Šæ¶¨',
            'summary': 'æ”¿åºœå®£å¸ƒæ–°æ”¿ç­–ï¼Œå¸‚åœºååº”ç§¯æ',
            'source': 'æµ‹è¯•æº',
            'published': datetime.now().isoformat(),
            'type': 'ç»æµã€æ”¿æ²»',
            'importance': 'é«˜'
        },
        {
            'title': 'ç§‘æŠ€å…¬å¸å‘å¸ƒæ–°äº§å“',
            'summary': 'é¢†å…ˆç§‘æŠ€å…¬å¸æ¨å‡ºåˆ›æ–°äº§å“',
            'source': 'æµ‹è¯•æº',
            'published': datetime.now().isoformat(),
            'type': 'ç§‘æŠ€',
            'importance': 'ä¸­'
        }
    ]
    
    trends = analyzer.analyze_news_trends(test_articles, hours=24)
    print(f"  åˆ†æå‘¨æœŸ: {trends['period']}")
    print(f"  æ–‡ç« æ€»æ•°: {trends['total_articles']}")
    print(f"  æƒ…æ„Ÿå€¾å‘: {trends['sentiment_analysis']['overall_sentiment']}")
    
    if trends.get('insights'):
        print("  è¶‹åŠ¿æ´å¯Ÿ:")
        for insight in trends['insights'][:3]:
            print(f"    â€¢ {insight}")
    
    print("\nâœ… è¶‹åŠ¿åˆ†æå™¨æµ‹è¯•å®Œæˆ")
    return True


if __name__ == "__main__":
    test_trend_analyzer()