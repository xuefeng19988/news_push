#!/usr/bin/env python3
"""
æ¸…ç†é‡å¤æ–‡ä»¶è„šæœ¬
ç§»é™¤åŠŸèƒ½é‡å¤çš„æ—§æ–‡ä»¶
"""

import os
import shutil
from pathlib import Path
from typing import List, Tuple

def identify_duplicate_files() -> List[Tuple[Path, str]]:
    """
    è¯†åˆ«é‡å¤æ–‡ä»¶
    
    Returns:
        é‡å¤æ–‡ä»¶åˆ—è¡¨ (æ–‡ä»¶è·¯å¾„, é‡å¤åŸå› )
    """
    src_dir = Path("src")
    duplicate_files = []
    
    # æ–‡ä»¶æ˜ å°„ï¼šæ–°æ–‡ä»¶ -> æ—§æ–‡ä»¶
    file_replacements = {
        # commonç›®å½•
        "auto_push_system_optimized_final.py": [
            "auto_push_system.py",
            "auto_push_system_optimized.py"
        ],
        "news_stock_pusher_optimized.py": [
            "news_stock_pusher.py"
        ],
        "base_pusher.py": [
            # è¿™ä¸ªæ–‡ä»¶æ˜¯åŸºç¡€ç±»ï¼Œä¸åˆ é™¤å…¶ä»–æ–‡ä»¶
        ],
        
        # newsç›®å½• - è¿™äº›åŠŸèƒ½å·²ç»è¢«æ•´åˆ
        "smart_pusher_enhanced.py": [
            "smart_pusher.py",
            "news_pusher.py",
            "global_news_pusher.py"
        ],
        
        # stocksç›®å½•
        "multi_stock_monitor.py": [
            "hourly_multi_stock_monitor.py",
            "hourly_alibaba_monitor.py",
            "auto_stock_notifier.py"
        ]
    }
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶æ ‡è®°ä¸ºé‡å¤
    for new_file, old_files in file_replacements.items():
        new_file_path = None
        
        # æŸ¥æ‰¾æ–°æ–‡ä»¶
        for found_file in src_dir.rglob(new_file):
            new_file_path = found_file
            break
        
        if new_file_path and new_file_path.exists():
            for old_file in old_files:
                for found_old_file in src_dir.rglob(old_file):
                    if found_old_file.exists() and found_old_file != new_file_path:
                        duplicate_files.append((found_old_file, f"è¢« {new_file_path.name} æ›¿ä»£"))
    
    # è¯†åˆ«åŠŸèƒ½é‡å¤çš„æ–‡ä»¶ï¼ˆé€šè¿‡æ–‡ä»¶åæ¨¡å¼ï¼‰
    file_patterns = {
        "optimized": "ä¼˜åŒ–ç‰ˆæœ¬å·²å­˜åœ¨",
        "enhanced": "å¢å¼ºç‰ˆæœ¬å·²å­˜åœ¨",
        "simple": "ç®€åŒ–ç‰ˆæœ¬å·²å­˜åœ¨",
        "backup": "å¤‡ä»½æ–‡ä»¶",
        "old": "æ—§ç‰ˆæœ¬æ–‡ä»¶",
        "test": "æµ‹è¯•æ–‡ä»¶ï¼ˆå¯ç§»åŠ¨åˆ°testsç›®å½•ï¼‰"
    }
    
    for keyword, reason in file_patterns.items():
        for py_file in src_dir.rglob(f"*.py"):
            if keyword in py_file.name.lower():
            if py_file.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„éä¼˜åŒ–ç‰ˆæœ¬
                base_name = py_file.stem
                if any(keyword in base_name for keyword in ["optimized", "enhanced", "simple", "backup", "old"]):
                    # æŸ¥æ‰¾å¯¹åº”çš„åŸºç¡€ç‰ˆæœ¬
                    base_version = base_name
                    for keyword in ["optimized", "enhanced", "simple", "backup", "old", "test"]:
                        base_version = base_version.replace(keyword, "").replace("_", "").strip("_")
                    
                    if base_version:
                        for base_file in src_dir.rglob(f"*{base_version}*.py"):
                            if base_file.exists() and base_file != py_file:
                                duplicate_files.append((py_file, f"{reason}: {base_file.name} å·²å­˜åœ¨"))
                                break
    
    return duplicate_files

def backup_file(file_path: Path) -> Path:
    """
    å¤‡ä»½æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        å¤‡ä»½æ–‡ä»¶è·¯å¾„
    """
    backup_dir = Path("backup_removed_files")
    backup_dir.mkdir(exist_ok=True)
    
    backup_path = backup_dir / file_path.name
    
    # å¦‚æœå¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ•°å­—åç¼€
    counter = 1
    while backup_path.exists():
        backup_path = backup_dir / f"{file_path.stem}_{counter}{file_path.suffix}"
        counter += 1
    
    shutil.copy2(file_path, backup_path)
    return backup_path

def cleanup_duplicate_files(dry_run: bool = True) -> Tuple[int, List[Tuple[Path, Path, str]]]:
    """
    æ¸…ç†é‡å¤æ–‡ä»¶
    
    Args:
        dry_run: æ˜¯å¦å¹²è¿è¡Œï¼ˆåªæ˜¾ç¤ºä¸åˆ é™¤ï¼‰
        
    Returns:
        Tuple[æ¸…ç†æ–‡ä»¶æ•°, æ¸…ç†è¯¦æƒ…åˆ—è¡¨]
    """
    duplicate_files = identify_duplicate_files()
    
    if not duplicate_files:
        print("âœ… æœªå‘ç°é‡å¤æ–‡ä»¶")
        return 0, []
    
    cleaned_files = []
    
    print(f"ğŸ” å‘ç° {len(duplicate_files)} ä¸ªé‡å¤/æ—§æ–‡ä»¶:")
    print("-" * 60)
    
    for file_path, reason in duplicate_files:
        print(f"  â€¢ {file_path}")
        print(f"    åŸå› : {reason}")
        print(f"    å¤§å°: {file_path.stat().st_size} å­—èŠ‚")
        print()
    
    print("-" * 60)
    
    if dry_run:
        print("ğŸ’¡ è¿™æ˜¯å¹²è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨ --execute å‚æ•°å®é™…æ‰§è¡Œæ¸…ç†")
        return 0, []
    
    # å®é™…æ¸…ç†
    print("ğŸ—‘ï¸ å¼€å§‹æ¸…ç†é‡å¤æ–‡ä»¶...")
    
    for file_path, reason in duplicate_files:
        try:
            # å¤‡ä»½æ–‡ä»¶
            backup_path = backup_file(file_path)
            
            # åˆ é™¤æ–‡ä»¶
            file_path.unlink()
            
            cleaned_files.append((file_path, backup_path, reason))
            
            print(f"  âœ… å·²æ¸…ç†: {file_path}")
            print(f"     å¤‡ä»½åˆ°: {backup_path}")
            print(f"     åŸå› : {reason}")
            
        except Exception as e:
            print(f"  âŒ æ¸…ç†å¤±è´¥ {file_path}: {e}")
    
    return len(cleaned_files), cleaned_files

def update_imports_after_cleanup(cleaned_files: List[Tuple[Path, Path, str]]):
    """
    æ¸…ç†åæ›´æ–°å¯¼å…¥è¯­å¥
    
    Args:
        cleaned_files: å·²æ¸…ç†çš„æ–‡ä»¶åˆ—è¡¨
    """
    if not cleaned_files:
        return
    
    print("\nğŸ”„ æ›´æ–°å¯¼å…¥è¯­å¥...")
    
    # æ”¶é›†è¢«åˆ é™¤çš„æ–‡ä»¶å
    removed_files = {file_path.stem: file_path for file_path, _, _ in cleaned_files}
    
    # æ›´æ–°æ‰€æœ‰Pythonæ–‡ä»¶
    src_dir = Path("src")
    updated_count = 0
    
    for py_file in src_dir.rglob("*.py"):
        if "__pycache__" in str(py_file):
            continue
        
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            
            # æ£€æŸ¥æ˜¯å¦å¯¼å…¥äº†è¢«åˆ é™¤çš„æ–‡ä»¶
            for removed_stem, removed_path in removed_files.items():
                import_patterns = [
                    f"from .*{removed_stem} import",
                    f"import .*{removed_stem}",
                    f"from {removed_stem} import",
                    f"import {removed_stem}"
                ]
                
                for pattern in import_patterns:
                    if pattern in content:
                        print(f"  âš ï¸  {py_file.name} å¯¼å…¥äº†å·²åˆ é™¤çš„æ¨¡å—: {removed_stem}")
                        # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªåŠ¨æ›¿æ¢é€»è¾‘ï¼Œä½†ä¸ºäº†å®‰å…¨ï¼Œæš‚æ—¶åªè­¦å‘Š
                        break
            
            if content != original_content:
                with open(py_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                updated_count += 1
                
        except Exception as e:
            print(f"  âŒ æ›´æ–°å¯¼å…¥å¤±è´¥ {py_file}: {e}")
    
    if updated_count > 0:
        print(f"âœ… æ›´æ–°äº† {updated_count} ä¸ªæ–‡ä»¶çš„å¯¼å…¥è¯­å¥")
    else:
        print("âœ… æ— éœ€æ›´æ–°å¯¼å…¥è¯­å¥")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¸…ç†é‡å¤æ–‡ä»¶")
    parser.add_argument("--execute", action="store_true", help="å®é™…æ‰§è¡Œæ¸…ç†ï¼ˆé»˜è®¤æ˜¯å¹²è¿è¡Œï¼‰")
    parser.add_argument("--update-imports", action="store_true", help="æ¸…ç†åæ›´æ–°å¯¼å…¥è¯­å¥")
    
    args = parser.parse_args()
    
    print("ğŸ§¹ é‡å¤æ–‡ä»¶æ¸…ç†å·¥å…·")
    print("=" * 60)
    
    # æ¸…ç†é‡å¤æ–‡ä»¶
    cleaned_count, cleaned_files = cleanup_duplicate_files(dry_run=not args.execute)
    
    if args.execute and cleaned_count > 0:
        print(f"\nğŸ“Š æ¸…ç†å®Œæˆ: ç§»é™¤äº† {cleaned_count} ä¸ªæ–‡ä»¶")
        
        # ä¿å­˜æ¸…ç†è®°å½•
        with open("file_cleanup_report.txt", "w", encoding="utf-8") as f:
            f.write("ğŸ—‘ï¸ æ–‡ä»¶æ¸…ç†æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ¸…ç†æ—¶é—´: {Path('.').resolve().name}\n")
            f.write(f"æ¸…ç†æ–‡ä»¶æ•°: {cleaned_count}\n\n")
            
            if cleaned_files:
                f.write("å·²æ¸…ç†çš„æ–‡ä»¶:\n")
                for file_path, backup_path, reason in cleaned_files:
                    f.write(f"  â€¢ {file_path}\n")
                    f.write(f"    å¤‡ä»½: {backup_path}\n")
                    f.write(f"    åŸå› : {reason}\n\n")
        
        print(f"ğŸ“ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: file_cleanup_report.txt")
        
        # æ›´æ–°å¯¼å…¥è¯­å¥
        if args.update_imports:
            update_imports_after_cleanup(cleaned_files)
    
    print("\nâœ… æ¸…ç†å·¥å…·æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()